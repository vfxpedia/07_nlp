{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b590af2",
   "metadata": {},
   "source": [
    "# 토큰화 (Tokenization)\n",
    "\n",
    "- 문장이나 단얼ㄹ 더 작은 단위로 나누어 분석 가능한 단위(토큰, Token)으로 변환하는 과정\n",
    "- 토큰의 다뉘가 상황에 따라 다르지만, 보통 의미있는 혹은 처리하는 단위로써 토큰 정의\n",
    "- 자연어 처리에서 크롤링, 데이터 수집 등으로 얻은 코퍼스 데이터는 정제되지 않은 경우가 많은데 이를 사용 용도에 맞게 토큰화, 정제, 정규화하는 과정이 필요\n",
    "\n",
    "**토큰화 목적**\n",
    "\n",
    "- 문법적 구조 이해\n",
    "- 유연한 데이터 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5814aa8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"NLP is fascinating. It has many application in real-world scenarios.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0e85fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 토큰화: ['NLP', 'is', 'fascinating', '.', 'It', 'has', 'many', 'application', 'in', 'real-world', 'scenarios', '.']\n",
      "문장 토큰화: ['NLP is fascinating.', 'It has many application in real-world scenarios.']\n",
      "문장별 단어 토큰화: ['NLP', 'is', 'fascinating', '.']\n",
      "문장별 단어 토큰화: ['It', 'has', 'many', 'application', 'in', 'real-world', 'scenarios', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# 단어 토큰화\n",
    "tokens = nltk.word_tokenize(text)\n",
    "print('단어 토큰화:', tokens)\n",
    "\n",
    "# 문장 토큰화\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "print('문장 토큰화:', sentences)\n",
    "\n",
    "# 문장별 단어 토큰화\n",
    "for sentence in sentences:\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    print('문장별 단어 토큰화:', tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a6e82e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NLP', 'is', 'fascinating', '.', 'It', 'has', 'many', 'application', 'in', 'real-world', 'scenarios', '.']\n",
      "['NLP is fascinating.', 'It has many application in real-world scenarios.']\n",
      "['NLP', 'is', 'fascinating', '.']\n",
      "['It', 'has', 'many', 'application', 'in', 'real-world', 'scenarios', '.']\n"
     ]
    }
   ],
   "source": [
    "# 단어 토큰화\n",
    "print(nltk.word_tokenize(text))\n",
    "\n",
    "# 문장 토큰화\n",
    "print(nltk.sent_tokenize(text))\n",
    "\n",
    "# 문장별 단어 토큰화\n",
    "for sent in nltk.sent_tokenize(text):\n",
    "    print(nltk.word_tokenize(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84652596",
   "metadata": {},
   "source": [
    "### Subword Tokenizing\n",
    "\n",
    "- BertTokenizer\n",
    "    - 단어를 부분 단위로 쪼개어 희귀하거나 새로운 단어도 부분적으로 표현할 수 있도록 함 → 어휘 크기를 줄이고 다양한 언어 패턴 학습 기능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d0639ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\playdata\\anaconda3\\envs\\ml_env\\lib\\site-packages (4.57.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\playdata\\anaconda3\\envs\\ml_env\\lib\\site-packages (from transformers) (3.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\playdata\\anaconda3\\envs\\ml_env\\lib\\site-packages (from transformers) (0.35.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\playdata\\anaconda3\\envs\\ml_env\\lib\\site-packages (from transformers) (2.3.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\playdata\\anaconda3\\envs\\ml_env\\lib\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\playdata\\anaconda3\\envs\\ml_env\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\playdata\\anaconda3\\envs\\ml_env\\lib\\site-packages (from transformers) (2025.9.1)\n",
      "Requirement already satisfied: requests in c:\\users\\playdata\\anaconda3\\envs\\ml_env\\lib\\site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\playdata\\anaconda3\\envs\\ml_env\\lib\\site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\playdata\\anaconda3\\envs\\ml_env\\lib\\site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\playdata\\anaconda3\\envs\\ml_env\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\playdata\\anaconda3\\envs\\ml_env\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\playdata\\anaconda3\\envs\\ml_env\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\playdata\\anaconda3\\envs\\ml_env\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\playdata\\anaconda3\\envs\\ml_env\\lib\\site-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\playdata\\anaconda3\\envs\\ml_env\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\playdata\\anaconda3\\envs\\ml_env\\lib\\site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\playdata\\anaconda3\\envs\\ml_env\\lib\\site-packages (from requests->transformers) (2025.8.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "add85e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['un', '##ha', '##pp', '##iness']\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# → Hugging Face 토큰화 모델 사용\n",
    "# Hugging Face란 : 머신러닝 모델 및 데이터셋을 쉽게 공유할 수 있는 플랫폼\n",
    "\n",
    "# 사전 학습된 형태의 토크나이저를 가져오겠다.\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "word = 'unhappiness'\n",
    "subwords = tokenizer.tokenize(word)\n",
    "print(subwords)\n",
    "# 결과 설명:\n",
    "# BertTokenizer를 처음 불러올 때, 내부적으로 여러 파일(vocab.txt, config.json 등)이 로드됩니다.\n",
    "# 이 중 vocab.txt에는 모델이 인식할 수 있는 모든 서브워드(subword)들이 저장되어 있습니다.\n",
    "# 'unhappiness'를 토크나이즈하면, 'un'은 vocab.txt에 독립적으로 존재하는 서브워드이기 때문에 그대로 출력됩니다.\n",
    "# 이어지는 'ha', 'pp', 'iness'는 단독 단어가 아닌, 접두어나 접미사로서 기존 단어들과 연결되어 등장할 수 있도록 '##'가 앞에 붙어 출력됩니다.\n",
    "# 이 '##' 표시는 해당 토큰이 앞 단어에 이어 붙는 부분임을 의미합니다.\n",
    "# 즉, ['un', '##ha', '##pp', '##iness']와 같은 결과가 나오는 이유는, BERT의 워드피스(WordPiece) 방식이 희귀 단어도 부분적으로 나눠 어휘 크기를 줄이고 처리할 수 있게 설계되었기 때문입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f54af6dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nl',\n",
       " '##p',\n",
       " 'is',\n",
       " 'fascinating',\n",
       " '.',\n",
       " 'it',\n",
       " 'has',\n",
       " 'many',\n",
       " 'application',\n",
       " 'in',\n",
       " 'real',\n",
       " '-',\n",
       " 'world',\n",
       " 'scenarios',\n",
       " '.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f60fdc",
   "metadata": {},
   "source": [
    "##### 문자 단위 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85cd6024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['H', 'e', 'l', 'l', 'o', ',', ' ', 'w', 'o', 'r', 'l', 'd', '!']\n"
     ]
    }
   ],
   "source": [
    "# 파이썬 기초에서 배운 문자 단위 토큰화 방법 : list() → 문자 단위로 토큰화\n",
    "text = \"Hello, world!\"\n",
    "tokens = list(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d826660",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Time',\n",
       " 'files',\n",
       " 'like',\n",
       " 'an',\n",
       " 'arrow',\n",
       " 'fruit',\n",
       " 'flies',\n",
       " 'like',\n",
       " 'a',\n",
       " 'banana']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# regular expression → 정규표현식을 사용하여 문자 단위로 토큰화\n",
    "\n",
    "import re\n",
    "text = \"Time files like an arrow; fruit flies like a banana\"\n",
    "re.findall(r'\\b\\w+\\b', text)\n",
    "\n",
    "# r'' : 파이썬 raw text (이스케이핑 문자를 문자 그대로 처리)\n",
    "# \\b  : 경계문자 (boundary / 공백, 구두점, ...)\n",
    "# \\w  : 글자 (word / 영문자, 숫자, _)\n",
    "# \\w+ : 수량자 (하나 이상)\n",
    "# => 경계 문자로 감싸인 하나 이상의 글자를 전부 찾아라."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "973a2204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'like', 'apples', 'oranges', 'and', 'bananas']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 아래는 정규표현식을 사용해서 영어 문장에서 단어(글자 덩어리)만 뽑아내는 코드예요.\n",
    "# 아주 쉬운 설명: 문장에서 알파벳이나 숫자, 언더바(_)로 이루어진 단어만 골라줘요. \n",
    "# 단어 사이에 쉼표(,)나 마침표(.) 같은 기호들은 빼고, 진짜 '단어'만 모아주는 거예요!\n",
    "\n",
    "# 예시\n",
    "text = \"I like apples, oranges, and bananas!\"\n",
    "words = re.findall(r'\\b\\w+\\b', text)\n",
    "print(words)  # 결과: ['I', 'like', 'apples', 'oranges', 'and', 'bananas']\n",
    "\n",
    "# 이런 식으로 re.findall() 함수는 '문장에서 단어만 찾아서 리스트로 만들어줘!'라는 뜻이에요.\n",
    "\n",
    "# 추가 설명\n",
    "# - r'' : '이스케이프 문자(\\)를 그대로 써도 돼!'라는 뜻의 파이썬 기능이에요. 즉, 백슬래시(\\)가 특수문자로 인식되지 않고 문자 그대로 쓰입니다.\n",
    "# - \\b : '단어의 경계'를 의미해요. 즉, 공백이나 구두점(.,! 등)과 단어 사이를 구분해줘요.\n",
    "# - \\w : 'word(단어) 문자'를 뜻하는데, 알파벳, 숫자, 언더스코어(_)를 포함합니다.\n",
    "# - \\w+ : +는 '하나 이상'이라는 뜻이에요. 즉, 하나 이상의 연속된 '단어 문자'(알파벳, 숫자, 언더스코어)를 모두 모아서 단어로 인식하는 것이죠!\n",
    "# 예를 들어, r'\\b\\w+\\b'는 \"경계에서부터 시작되는 하나 이상의 단어문자(알파벳/숫자/_)로 이루어진 부분들을 모두 찾아줘!\"라는 뜻이랍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d16dd76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Don', \"'\", 't', 'hesitate', 'to', 'use', 'well', '-', 'being', 'practices', 'for', 'self', '-', 'care', '.']\n",
      "['Do', \"n't\", 'hesitate', 'to', 'use', 'well-being', 'practices', 'for', 'self-care', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer, word_tokenize\n",
    "\n",
    "text = \"Don't hesitate to use well-being practices for self-care.\"\n",
    "\n",
    "# WordPuncttokenizer: 단어/구두점으로 토큰을 구분 (', - 포함 단어도 분리)\n",
    "word_punct_tokenizer = WordPunctTokenizer()\n",
    "print(word_punct_tokenizer.tokenize(text))\n",
    "\n",
    "print(word_tokenize(text))\n",
    "\n",
    "# # 예시\n",
    "# text = \"I like apples, oranges, and bananas!\"\n",
    "# words = re.findall(r'\\b\\w+\\b', text)\n",
    "# print(words)  # 결과: ['I', 'like', 'apples', 'oranges', 'and', 'bananas']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67f8fc73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['COVID-19', '(', '전염병', ')', ',', 'Dr.Smith', '(', '의사', ')', ',', 'NASA', '(', '우주항공국', ')', '등', '특정', '기관이나', '명칭이', '있다.', '특수', '문자', '또한', '태그', '<', 'br', '>', ',', '가격', '$', '100.50', ',', '2025/02/18', '날짜표현에', '사용될', '수', '있다.', '이러한', '경우', ',', '$', '100.50을', '하나의', '토큰으로', '유지할', '필요가', '있다', '.']\n",
      "['COVID-19', '(', '전염병', ')', ',', 'Dr.Smith', '(', '의사', ')', ',', 'NASA', '(', '우주항공국', ')', '등', '특정', '기관이나', '명칭이', '있다', '.', '특수', '문자', '또한', '태그', '<', 'br', '>', ',', '가격', '$', '100.50', ',', '2025/02/18', '날짜표현에', '사용될', '수', '있다', '.', '이러한', '경우', ',', '$', '100.50을', '하나의', '토큰으로', '유지할', '필요가', '있다', '.']\n"
     ]
    }
   ],
   "source": [
    "# TreebankWordTokenizer는 Penn Treebank 코퍼스의 tokenization 규칙을 따르는 토크나이저입니다.\n",
    "# 이 토크나이저는 표준적인 영어 어절의 분리뿐만 아니라, 하이픈, 구두점, 약어, 숫자와 기호(쉼표, 달러표시, 온점 등)처럼\n",
    "# 복잡한 패턴도 적절히 처리하여 토큰을 나눕니다.\n",
    "# 예를 들어, 약어(\"Dr.Smith\"), 화폐 단위(\"$100.50\"), 날짜(\"2025/02/18\")와 같이 \n",
    "# 일반적인 단어 경계로 단순 분리하기 어려운 케이스도 Treebank 규칙에 맞게 토큰화해줍니다.\n",
    "\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "text = '''\n",
    "COVID-19(전염병), Dr.Smith(의사), NASA(우주항공국) 등 특정 기관이나 명칭이 있다. \n",
    "특수 문자 또한 태그 <br>, 가격 $100.50, 2025/02/18 날짜표현에 사용될 수 있다. \n",
    "이러한 경우, $100.50을 하나의 토큰으로 유지할 필요가 있다.\n",
    "'''\n",
    "\n",
    "treebank_tokenizer = TreebankWordTokenizer()\n",
    "print(treebank_tokenizer.tokenize(text))\n",
    "\n",
    "print(word_tokenize(text))\n",
    "\n",
    "# treebank_tokenizer와 word_tokenize의 토큰화 결과는 비슷하지만 약간씩 차이가 있습니다.\n",
    "# 예를 들어, treebank_tokenizer는 약어, 하이픈, 구두점 등 Penn Treebank의 세부 규칙을 따르기 때문에\n",
    "# 일부 특수케이스(예: \"Dr.Smith\", \"$100.50\", 날짜, 괄호 등)에 조금 더 세밀하게 토큰을 나눕니다.\n",
    "# 반면 word_tokenize는 내부적으로 TreebankWordTokenizer를 기반으로 하지만,\n",
    "# 전처리 및 언어 모델링 용도로 조금 다르게 동작할 수 있습니다(예: 줄바꿈, 특수문자 처리 등).\n",
    "# 실제 출력된 토큰을 보면, 둘 다 쉼표와 온점 등 구두점을 별도 토큰으로 분리했지만,\n",
    "# 괄호나 특수기호에 따라 토큰화 결과에 차이가 있을 수 있습니다.\n",
    "# 따라서 텍스트 처리 목적(정밀한 문법단위 분할 vs. 일반적 단어 분리 등)에 따라\n",
    "# 두 토크나이저 중 적합한 것을 선택하는 것이 좋겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb51bbd",
   "metadata": {},
   "source": [
    "### 영어와 한국어 토큰화 및 형태소 분석의 차이점\n",
    "\n",
    "- **영어(English)**\n",
    "    - 영어는 띄어쓰기를 기준으로 대부분의 어절(token)이 분리됩니다.\n",
    "    - NLTK, spaCy 같은 라이브러리의 `word_tokenize`와 같은 함수로 비교적 쉽게 단어 단위로 분리가 가능합니다.\n",
    "    - 영어의 형태소 분석(morphological analysis)은 대부분 단어의 원형(lemma) 추출이나 접두/접미어 처리에 초점을 맞춥니다.\n",
    "\n",
    "- **한국어(Korean)**\n",
    "    - 한국어는 교착어(Agglutinative language)로, 한 어절에 여러 문법적 의미(어간+어미+조사 등)가 결합되어 있습니다.\n",
    "    - 띄어쓰기만으로 정확한 토큰 분리가 어렵고, **형태소 분석기**가 필요합니다.\n",
    "    - 대표적인 형태소 분석 라이브러리로는 [KoNLPy](https://konlpy.org/)의 `Mecab`, `Okt`, `Kkma`, `Komoran` 등이 있습니다.\n",
    "    - 형태소 분석기는 각 단어를 다시 품사, 어근 등 세부 단위(morpheme)로 쪼개고 품사 태깅을 제공합니다.\n",
    "\n",
    "#### 주요 차이점 요약\n",
    "\n",
    "| 구분      | 영어                        | 한국어                         |\n",
    "|-----------|----------------------------|--------------------------------|\n",
    "| 토큰 기준 | 띄어쓰기, 구두점            | 띄어쓰기 + 형태소(어미, 조사)  |\n",
    "| 처리 도구 | NLTK, spaCy 등              | KoNLPy, Mecab, Okt 등          |\n",
    "| 라이브러리 설치 | 기본적으로 NLTK 등으로 충분 | 추가로 KoNLPy, 형태소 분석기 필요 |\n",
    "\n",
    "#### 한국어 형태소 분석기 설치 예시(KoNLPy + Okt):\n",
    "\n",
    "```python\n",
    "# pip로 설치\n",
    "!pip install konlpy\n",
    "\n",
    "# 사용 예시\n",
    "from konlpy.tag import Okt\n",
    "okt = Okt()\n",
    "print(okt.morphs(\"한국어 형태소 분석은 재미있어요!\"))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dab04bf",
   "metadata": {},
   "source": [
    "### 한국어 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3099a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kss==5.0.0 in c:\\users\\playdata\\anaconda3\\envs\\ml_env\\lib\\site-packages (5.0.0)\n",
      "Requirement already satisfied: emoji==1.2.0 in c:\\users\\playdata\\anaconda3\\envs\\ml_env\\lib\\site-packages (from kss==5.0.0) (1.2.0)\n",
      "Requirement already satisfied: regex in c:\\users\\playdata\\anaconda3\\envs\\ml_env\\lib\\site-packages (from kss==5.0.0) (2025.9.1)\n",
      "Requirement already satisfied: pecab in c:\\users\\playdata\\anaconda3\\envs\\ml_env\\lib\\site-packages (from kss==5.0.0) (1.0.8)\n",
      "Requirement already satisfied: networkx in c:\\users\\playdata\\anaconda3\\envs\\ml_env\\lib\\site-packages (from kss==5.0.0) (3.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\playdata\\anaconda3\\envs\\ml_env\\lib\\site-packages (from pecab->kss==5.0.0) (2.3.3)\n",
      "Requirement already satisfied: pyarrow in c:\\users\\playdata\\anaconda3\\envs\\ml_env\\lib\\site-packages (from pecab->kss==5.0.0) (21.0.0)\n",
      "Requirement already satisfied: pytest in c:\\users\\playdata\\anaconda3\\envs\\ml_env\\lib\\site-packages (from pecab->kss==5.0.0) (8.4.2)\n",
      "Requirement already satisfied: colorama>=0.4 in c:\\users\\playdata\\anaconda3\\envs\\ml_env\\lib\\site-packages (from pytest->pecab->kss==5.0.0) (0.4.6)\n",
      "Requirement already satisfied: iniconfig>=1 in c:\\users\\playdata\\anaconda3\\envs\\ml_env\\lib\\site-packages (from pytest->pecab->kss==5.0.0) (2.1.0)\n",
      "Requirement already satisfied: packaging>=20 in c:\\users\\playdata\\anaconda3\\envs\\ml_env\\lib\\site-packages (from pytest->pecab->kss==5.0.0) (25.0)\n",
      "Requirement already satisfied: pluggy<2,>=1.5 in c:\\users\\playdata\\anaconda3\\envs\\ml_env\\lib\\site-packages (from pytest->pecab->kss==5.0.0) (1.6.0)\n",
      "Requirement already satisfied: pygments>=2.7.2 in c:\\users\\playdata\\anaconda3\\envs\\ml_env\\lib\\site-packages (from pytest->pecab->kss==5.0.0) (2.19.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install kss==5.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "867bab92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Kss]: Because there's no supported C++ morpheme analyzer, Kss will take pecab as a backend. :D\n",
      "For your information, Kss also supports mecab backend.\n",
      "We recommend you to install mecab or konlpy.tag.Mecab for faster execution of Kss.\n",
      "Please refer to following web sites for details:\n",
      "- mecab: https://cleancode-ws.tistory.com/97\n",
      "- konlpy.tag.Mecab: https://uwgdqo.tistory.com/363\n",
      "\n",
      "c:\\Users\\Playdata\\anaconda3\\envs\\ml_env\\Lib\\site-packages\\pecab\\_tokenizer.py:265: RuntimeWarning: overflow encountered in scalar add\n",
      "  from_pos_data.costs[idx]\n",
      "c:\\Users\\Playdata\\anaconda3\\envs\\ml_env\\Lib\\site-packages\\pecab\\_tokenizer.py:274: RuntimeWarning: overflow encountered in scalar add\n",
      "  least_cost += word_cost\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['배경은 1920년대의 경성부이다.',\n",
       " '주인공이자 인력거꾼 김 첨지의 아내는 병에 걸린 지 1달 가량이 지나 있었다.',\n",
       " \"아내는 단 한 번도 약을 먹어본 적이 없는데, 그 이유는 '병이란 놈에게 약을 주어 보내면 재미를 붙여서 자꾸 온다.'는 김 첨지의 신조 때문.\",\n",
       " '멍청이, 꼰대...가 아닐수 없다.',\n",
       " '사실 이건 핑계고, 약을 살 돈도 벌지 못하고 있었다는 이유가 더 크다.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# kss (Korean Sentence Splitter)\n",
    "# 한국어 문장 또는 한국어/ 영어 혼합 문장 등에 문장단위 토큰 지원\n",
    "import kss\n",
    "\n",
    "text = \"배경은 1920년대의 경성부이다. 주인공이자 인력거꾼 김 첨지의 아내는 병에 걸린 지 1달 가량이 지나 있었다. 아내는 단 한 번도 약을 먹어본 적이 없는데, 그 이유는 '병이란 놈에게 약을 주어 보내면 재미를 붙여서 자꾸 온다.'는 김 첨지의 신조 때문. 멍청이, 꼰대...가 아닐수 없다. 사실 이건 핑계고, 약을 살 돈도 벌지 못하고 있었다는 이유가 더 크다.\"\n",
    "\n",
    "kss.split_sentences(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9f4320",
   "metadata": {},
   "source": [
    "### 품사 태깅\n",
    "\n",
    "**pos_tag**\n",
    "\n",
    "pos_tag는 자연어 처리(NLP)에서 단어에 품사를 태깅하는 함수로, 주로 NLTK와 같은 라이브러리에서 사용된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ec4670",
   "metadata": {},
   "source": [
    "- nltk pos_tag() 주요 품사 태깅<br>\n",
    "\n",
    "1. **NN (Noun, Singular)**  \n",
    "   단수 명사를 나타낸다. 하나의 사물이나 개념을 지칭한다.  \n",
    "   예시: \"cat\", \"book\", \"apple\"\n",
    "\n",
    "2. **NNS (Noun, Plural)**  \n",
    "   복수 명사를 나타낸다. 두 개 이상의 사물이나 개념을 지칭한다.  \n",
    "   예시: \"cats\", \"books\", \"apples\"\n",
    "\n",
    "3. **NNP (Proper Noun, Singular)**  \n",
    "   단수 고유 명사를 나타낸다. 특정한 사람, 장소 또는 조직의 이름을 지칭한다.  \n",
    "   예시: \"Alice\", \"London\", \"NASA\"\n",
    "\n",
    "4. **NNPS (Proper Noun, Plural)**  \n",
    "   복수 고유 명사를 나타낸다. 두 개 이상의 특정한 사람, 장소 또는 조직의 이름을 지칭한다.  \n",
    "   예시: \"Smiths\", \"United Nations\"\n",
    "\n",
    "5. **VB (Verb, Base Form)**  \n",
    "   동사의 원형을 나타낸다. 일반적으로 현재 시제와 함께 사용된다.  \n",
    "   예시: \"run\", \"eat\", \"play\"\n",
    "\n",
    "6. **VBD (Verb, Past Tense)**  \n",
    "   동사의 과거형을 나타낸다.  \n",
    "   예시: \"ran\", \"ate\", \"played\"\n",
    "\n",
    "7. **VBG (Verb, Gerund or Present Participle)**  \n",
    "   동명사 또는 현재 분사를 나타낸다. 일반적으로 \"-ing\" 형태이다.  \n",
    "   예시: \"running\", \"eating\", \"playing\"\n",
    "\n",
    "8. **VBN (Verb, Past Participle)**  \n",
    "   동사의 과거 분사형을 나타낸다. 주로 완료 시제와 함께 사용된다.  \n",
    "   예시: \"run\" (as in \"has run\"), \"eaten\", \"played\"\n",
    "\n",
    "9. **VBZ (Verb, 3rd Person Singular Present)**  \n",
    "   3인칭 단수 현재형 동사를 나타낸다. 주어가 3인칭 단수일 때 사용된다.  \n",
    "   예시: \"runs\", \"eats\", \"plays\"\n",
    "\n",
    "10. **JJ (Adjective)**  \n",
    "    형용사를 나타낸다. 명사를 수식하여 그 특성을 설명한다.  \n",
    "    예시: \"big\", \"blue\", \"happy\"\n",
    "\n",
    "11. **JJR (Adjective, Comparative)**  \n",
    "    비교급 형용사를 나타낸다. 두 개의 대상을 비교할 때 사용된다.  \n",
    "    예시: \"bigger\", \"bluer\", \"happier\"\n",
    "\n",
    "12. **JJS (Adjective, Superlative)**  \n",
    "    최상급 형용사를 나타낸다. 세 개 이상의 대상을 비교할 때 사용된다.  \n",
    "    예시: \"biggest\", \"bluest\", \"happiest\"\n",
    "\n",
    "13. **RB (Adverb)**  \n",
    "    부사를 나타낸다. 동사, 형용사 또는 다른 부사를 수식한다.  \n",
    "    예시: \"quickly\", \"very\", \"well\"\n",
    "\n",
    "14. **RBR (Adverb, Comparative)**  \n",
    "    비교급 부사를 나타낸다. 두 개의 대상을 비교할 때 사용된다.  \n",
    "    예시: \"more quickly\", \"better\"\n",
    "\n",
    "15. **RBS (Adverb, Superlative)**  \n",
    "    최상급 부사를 나타낸다. 세 개 이상의 대상을 비교할 때 사용된다.  \n",
    "    예시: \"most quickly\", \"best\"\n",
    "\n",
    "16. **IN (Preposition or Subordinating Conjunction)**  \n",
    "    전치사 또는 종속 접속사를 나타낸다. 명사와의 관계를 나타내거나 종속절을 시작한다.  \n",
    "    예시: \"in\", \"on\", \"because\"\n",
    "\n",
    "17. **DT (Determiner)**  \n",
    "    한정사를 나타낸다. 명사의 수와 상태를 정의한다.  \n",
    "    예시: \"the\", \"a\", \"some\"\n",
    "\n",
    "18. **PRP (Personal Pronoun)**  \n",
    "    인칭 대명사를 나타낸다. 사람, 사물 등을 대체할 때 사용된다.  \n",
    "    예시: \"I\", \"you\", \"he\", \"they\"\n",
    "\n",
    "19. **PRP$ (Possessive Pronoun)**  \n",
    "    소유 대명사를 나타낸다. 소유 관계를 나타낸다.  \n",
    "    예시: \"my\", \"your\", \"his\", \"their\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a52719e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\Playdata\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "716c5847",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Time', 'NNP'),\n",
       " ('flies', 'NNS'),\n",
       " ('like', 'IN'),\n",
       " ('an', 'DT'),\n",
       " ('arrow', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag import pos_tag\n",
    "\n",
    "text = \"Time flies like an arrow.\"\n",
    "tokens = word_tokenize(text)\n",
    "pos_tag = pos_tag(tokens)\n",
    "pos_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3b6c0c",
   "metadata": {},
   "source": [
    "**spacy 주요 품사 태깅**\n",
    "\n",
    "| 태그 | 설명                 | 예시                 |\n",
    "|------|----------------------|----------------------|\n",
    "| ADJ  | 형용사               | big, nice           |\n",
    "| ADP  | 전치사               | in, to, on          |\n",
    "| ADV  | 부사                 | very, well          |\n",
    "| AUX  | 조동사               | is, have (조동사로 사용될 때) |\n",
    "| CONJ | 접속사               | and, or             |\n",
    "| DET  | 한정사/관사          | the, a              |\n",
    "| INTJ | 감탄사               | oh, wow             |\n",
    "| NOUN | 명사                 | dog, table          |\n",
    "| NUM  | 숫자                 | one, two, 3         |\n",
    "| PART | 소사                 | 'to' (to fly에서), not |\n",
    "| PRON | 대명사               | he, she, it         |\n",
    "| PROPN| 고유명사             | John, France        |\n",
    "| PUNCT| 구두점               | ., !, ?             |\n",
    "| SCONJ| 종속 접속사          | because, if         |\n",
    "| SYM  | 기호                 | $, %, @             |\n",
    "| VERB | 동사                 | run, eat            |\n",
    "| X    | 알 수 없는 품사       | 외국어 단어, 잘못된 형식 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ab1d157e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\playdata\\anaconda3\\envs\\ml_env\\lib\\site-packages (3.8.7)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\playdata\\anaconda3\\envs\\ml_env\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\playdata\\anaconda3\\envs\\ml_env\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\playdata\\anaconda3\\envs\\ml_env\\lib\\site-packages (from spacy) (1.0.13)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\playdata\\anaconda3\\envs\\ml_env\\lib\\site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\playdata\\anaconda3\\envs\\ml_env\\lib\\site-packages (from spacy) (3.0.10)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in c:\\users\\playdata\\anaconda3\\envs\\ml_env\\lib\\site-packages (from spacy) (8.3.6)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\playdata\\anaconda3\\envs\\ml_env\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\playdata\\anaconda3\\envs\\ml_env\\lib\\site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\playdata\\anaconda3\\envs\\ml_env\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\playdata\\anaconda3\\envs\\ml_env\\lib\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\playdata\\anaconda3\\envs\\ml_env\\lib\\site-packages (from spacy) (0.19.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\playdata\\anaconda3\\envs\\ml_env\\lib\\site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\playdata\\anaconda3\\envs\\ml_env\\lib\\site-packages (from spacy) (2.3.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\playdata\\anaconda3\\envs\\ml_env\\lib\\site-packages (from spacy) (2.32.5)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\playdata\\anaconda3\\envs\\ml_env\\lib\\site-packages (from spacy) (2.12.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\playdata\\anaconda3\\envs\\ml_env\\lib\\site-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\playdata\\anaconda3\\envs\\ml_env\\lib\\site-packages (from spacy) (78.1.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\playdata\\anaconda3\\envs\\ml_env\\lib\\site-packages (from spacy) (25.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\playdata\\anaconda3\\envs\\ml_env\\lib\\site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\playdata\\anaconda3\\envs\\ml_env\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\playdata\\anaconda3\\envs\\ml_env\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in c:\\users\\playdata\\anaconda3\\envs\\ml_env\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.41.4)\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in c:\\users\\playdata\\anaconda3\\envs\\ml_env\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\playdata\\anaconda3\\envs\\ml_env\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\playdata\\anaconda3\\envs\\ml_env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\playdata\\anaconda3\\envs\\ml_env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\playdata\\anaconda3\\envs\\ml_env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\playdata\\anaconda3\\envs\\ml_env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.8.3)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in c:\\users\\playdata\\anaconda3\\envs\\ml_env\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\playdata\\anaconda3\\envs\\ml_env\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\playdata\\anaconda3\\envs\\ml_env\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\playdata\\anaconda3\\envs\\ml_env\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.2.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\playdata\\anaconda3\\envs\\ml_env\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\playdata\\anaconda3\\envs\\ml_env\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (14.2.0)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\playdata\\anaconda3\\envs\\ml_env\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.23.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\playdata\\anaconda3\\envs\\ml_env\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.3.1)\n",
      "Requirement already satisfied: wrapt in c:\\users\\playdata\\anaconda3\\envs\\ml_env\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.3)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in c:\\users\\playdata\\anaconda3\\envs\\ml_env\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.3.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\playdata\\anaconda3\\envs\\ml_env\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\playdata\\anaconda3\\envs\\ml_env\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\playdata\\anaconda3\\envs\\ml_env\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\playdata\\anaconda3\\envs\\ml_env\\lib\\site-packages (from jinja2->spacy) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6513376c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "spacy.cli.download('en_core_web_sm')       # 영어 모델 다운로드 (사용 전 1회는 반드시 다운로드)\n",
    "spacy_nlp = spacy.load('en_core_web_sm')   # 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "762bf1ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time : NOUN\n",
      "flies : VERB\n",
      "like : ADP\n",
      "an : DET\n",
      "arrow : NOUN\n",
      ". : PUNCT\n"
     ]
    }
   ],
   "source": [
    "tokens = spacy_nlp(text)\n",
    "\n",
    "for token in tokens:\n",
    "    print(token.text, \":\", token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769cb2d9",
   "metadata": {},
   "source": [
    "### KoNLPy 품사 태깅\n",
    "\n",
    "- 한국어 자연어 처리를 위한 라이브러리\n",
    "- 형태소 분석, 품사 태깅, 텍스트 전처리 등 기능 지원\n",
    "- 여러 형태소 분석기 중 적합한 분석기 선택 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d06ef9e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: konlpy in c:\\users\\playdata\\anaconda3\\envs\\ml_env\\lib\\site-packages (0.6.0)\n",
      "Requirement already satisfied: JPype1>=0.7.0 in c:\\users\\playdata\\anaconda3\\envs\\ml_env\\lib\\site-packages (from konlpy) (1.6.0)\n",
      "Requirement already satisfied: lxml>=4.1.0 in c:\\users\\playdata\\anaconda3\\envs\\ml_env\\lib\\site-packages (from konlpy) (6.0.2)\n",
      "Requirement already satisfied: numpy>=1.6 in c:\\users\\playdata\\anaconda3\\envs\\ml_env\\lib\\site-packages (from konlpy) (2.3.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\playdata\\anaconda3\\envs\\ml_env\\lib\\site-packages (from JPype1>=0.7.0->konlpy) (25.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install konlpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d68fce",
   "metadata": {},
   "source": [
    "### Java와 JDK는 다른가요? 그리고 konlpy.tag.Okt 사용 준비\n",
    "\n",
    " - **JVM(Java Virtual Machine)** 은 자바로 만든 프로그램을 실행하는 가상 머신입니다.\n",
    " - **JDK(Java Development Kit)** 는 자바 개발에 필요한 모든 도구가 들어 있는 패키지이고, 이 안에 JVM도 포함되어 있습니다.\n",
    "   - 즉, **JDK를 설치하면 JVM도 같이 설치** 됩니다.\n",
    "   - 자바 프로그램을 실행하려면 최소한 JVM이 필요하고, 보통 JDK를 설치해야 합니다.\n",
    "   - 예전에는 JRE(Java Runtime Environment)만으로도 실행이 가능했으나, 요즘은 대부분 JDK 설치가 권장/필요합니다.\n",
    " - **Java와 JDK의 차이점**\n",
    "   - \"Java\"는 언어 및 플랫폼 전체를 가리키는 용어로, 자바 언어(JVM, JDK 모두 포함)를 의미합니다.\n",
    "   - \"JDK\"는 자바로 코딩/컴파일/실행하는데 필요한 개발 키트(Package)입니다.\n",
    "   - *즉, 정리하면*: JDK = JVM + 개발도구들(컴파일러 등)\n",
    "\n",
    " - konlpy의 형태소 분석기 **Okt** 는 Java(정확히는 JDK / JVM)에 의존하므로, **JDK가 반드시 필요** 합니다.\n",
    " - 여러 JDK 중에는 최근 많이 사용되고, 무료/오픈소스로 신뢰도 높은 **Temurin JDK** (17버전 등 LTS 권장)를 사용하는 것이 좋습니다.\n",
    "\n",
    " - JDK 설치 및 환경변수 설정 방법 예시 (Windows 기준):\n",
    "     1. Temurin JDK 17 (예시: Temurin 17.0.16+8) 버전을 다운로드/설치합니다.\n",
    "         - 공식 사이트: https://adoptium.net/ 또는 https://adoptium.net/temurin/releases\n",
    "         - 설치 경로 예시: `C:\\SKN_19\\dev\\jdk-17.0.16+8`\n",
    "     2. **환경변수 설정**\n",
    "         - `Ctrl+R` → `sysdm.cpl ,3` 입력 (시스템 속성 – 고급 – 환경 변수)\n",
    "         - [시스템 변수]에서 \"새로 만들기\" 클릭\n",
    "         - 변수 이름: `JAVA_HOME`\n",
    "         - 변수 값: `C:\\SKN_19\\dev\\jdk-17.0.16+8`\n",
    "         - 확인\n",
    "     3. [시스템 변수]에서 `Path` 선택 → \"편집\"\n",
    "         - 새 항목 추가: `%JAVA_HOME%\\bin`\n",
    "         - (실행 우선순위 향상을 위해) 추가한 항목을 가장 위로 이동\n",
    "         - 확인\n",
    "     4. 환경변수 적용을 위해 **cmd 등 터미널을 새로 열어** 아래 명령어로 정상 인식되는지 확인합니다:\n",
    "         ```\n",
    "         echo %JAVA_HOME%\n",
    "         java -version\n",
    "         ```\n",
    "     5. 이제 konlpy 및 Okt 등 자바 기반 툴이 정상 동작해야 합니다.\n",
    "\n",
    " - 참고: Mecab 등 일부 형태소 분석기는 Java 대신 C++ 환경이 필요할 수 있습니다.\n",
    " - **정리**: Okt 등 konlpy 일부 모듈은 JDK(예: Temurin JDK)가 필요하고, JAVA_HOME 과 시스템 PATH 환경변수를 반드시 지정해주어야 합니다. Mecab 등은 C++ 기반입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8b035f99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['오늘', '점심', '은', '뭐', '먹어', '볼까', '.', '맛있는', '게', '뭐', '지', '?']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "text = \"오늘 점심은 뭐 먹어볼까. 맛있는 게 뭐지?\"\n",
    "okt = Okt()\n",
    "\n",
    "morphs = okt.morphs(text)\n",
    "morphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e8159354",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('오늘', 'Noun'),\n",
       " ('점심', 'Noun'),\n",
       " ('은', 'Josa'),\n",
       " ('뭐', 'Noun'),\n",
       " ('먹어', 'Verb'),\n",
       " ('볼까', 'Verb'),\n",
       " ('.', 'Punctuation'),\n",
       " ('맛있는', 'Adjective'),\n",
       " ('게', 'Noun'),\n",
       " ('뭐', 'Noun'),\n",
       " ('지', 'Josa'),\n",
       " ('?', 'Punctuation')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 품사 태깅\n",
    "pos_tags = okt.pos(text)\n",
    "pos_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "01778750",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['오늘', '점심', '뭐', '게', '뭐']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 명사 추출\n",
    "nouns = okt.nouns(text)\n",
    "nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "498d7d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "품사 태킹 : [('오늘', 'Noun'), ('점심', 'Noun'), ('은', 'Josa'), ('뭐', 'Noun'), ('먹', 'Verb'), ('엉', 'Exclamation'), ('볼까', 'Verb'), ('게', 'Noun'), ('.', 'Punctuation'), ('맛있는', 'Adjective'), ('거', 'Noun'), ('멘디', 'Noun'), ('?', 'Punctuation')]\n",
      "명사 추출 : ['오늘', '점심', '뭐', '게', '거', '멘디']\n"
     ]
    }
   ],
   "source": [
    "text_jeju = \"오늘 점심은 뭐 먹엉 볼까게. 맛있는 거 멘디?\"\n",
    "\n",
    "# 품사 태깅\n",
    "pos_tags = okt.pos(text_jeju)\n",
    "print('품사 태킹 :', pos_tags)\n",
    "\n",
    "# 명사 추출\n",
    "nouns = okt.nouns(text_jeju)\n",
    "print('명사 추출 :', nouns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
