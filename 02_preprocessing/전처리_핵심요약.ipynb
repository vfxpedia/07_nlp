{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 전처리 핵심 요약: 이론과 실습\n",
        "\n",
        "이 노트북은 텍스트 전처리의 핵심 개념과 최소 실행 예제를 한 곳에 정리해, 학습자가 바로 실행하며 복습할 수 있도록 구성했습니다.\n",
        "\n",
        "- 목적: 개념 고착, 실행 중심 복습, 이후 Subword 섹션 추가 시 기반 정리\n",
        "- 데이터: 간단한 예제 텍스트 중심 (대용량 파일은 미사용)\n",
        "- 실행 방식: 각 섹션의 코드 셀을 순서대로 실행\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 📚 목차\n",
        "1. 핵심 키워드 한눈에 보기\n",
        "2. 전처리 파이프라인 개요\n",
        "3. 텍스트 정규화와 토큰화\n",
        "4. 불용어 제거\n",
        "5. 정규표현식 핵심 패턴\n",
        "6. 정수 인코딩(Integer Encoding)\n",
        "7. 시퀀스 패딩(Padding)\n",
        "8. 원-핫 인코딩(One-Hot Encoding)\n",
        "9. 워드클라우드(Word Cloud)\n",
        "10. 체크리스트 & 실전 팁\n",
        "11. 복습 퀴즈 & 미니 과제\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) 핵심 키워드 한눈에 보기\n",
        "- **Token**: 텍스트의 기본 단위\n",
        "- **Normalization**: 대소문자 통일, 공백/특수문자 처리 등 규칙화\n",
        "- **Stopwords**: 분석에 덜 유의미한 단어들(예: the, and, 은/는/이/가)\n",
        "- **Stemming vs Lemmatization**: 어간 추출(형태만) vs 표제어 추출(사전기반 의미)\n",
        "- **Regex(정규표현식)**: 패턴 기반 치환/추출\n",
        "- **Integer Encoding**: 토큰을 정수로 매핑\n",
        "- **Padding**: 시퀀스 길이 표준화\n",
        "- **One-Hot**: 정수 레이블/토큰을 진리표 형태의 벡터로 표현\n",
        "- **OOV(Out-Of-Vocabulary)**: 학습 사전에 없는 단어 처리 전략\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) 전처리 파이프라인 개요\n",
        "1. 텍스트 정규화(소문자/공백/특수문자/숫자 처리)\n",
        "2. 토큰화\n",
        "3. 불용어 제거(도메인 맞춤 확장)\n",
        "4. 표제어/어간 처리(언어/도메인/모델 특성에 맞춰 선택)\n",
        "5. 정수 인코딩 → 패딩\n",
        "6. 필요 시 원-핫/임베딩 선택\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 공통 예제 텍스트와 유틸 함수\n",
        "sample_text = \"The Matrix is everywhere! It's all around us; here, even in this room.\"\n",
        "\n",
        "def show(title, obj):\n",
        "    print(f\"\\n[{title}]\\n{obj}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) 텍스트 정규화와 토큰화\n",
        "- 대소문자 통일, 구두점 제거 여부는 태스크에 맞게 결정\n",
        "- 토큰화는 언어/도메인별 토크나이저 선택(영어 예시 사용)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "text = sample_text\n",
        "lowered = text.lower()\n",
        "# 구두점 제거\n",
        "punc_table = str.maketrans('', '', string.punctuation)\n",
        "no_punc = lowered.translate(punc_table)\n",
        "\n",
        "show(\"원문\", text)\n",
        "show(\"소문자\", lowered)\n",
        "show(\"구두점 제거\", no_punc)\n",
        "show(\"단어 토큰화\", word_tokenize(no_punc))\n",
        "show(\"문장 토큰화\", sent_tokenize(text))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) 불용어 제거\n",
        "- 일반 불용어 + 도메인 커스텀 불용어 함께 사용 권장\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "\n",
        "# 필요 리소스 다운로드(최초 1회)\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "custom_stopwords = {\"it's\", \"us\", \"this\"}\n",
        "all_stopwords = stop_words.union(custom_stopwords)\n",
        "\n",
        "tokens = word_tokenize(no_punc)\n",
        "filtered = [t for t in tokens if t not in all_stopwords]\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized = [lemmatizer.lemmatize(t, pos='v') for t in filtered]\n",
        "\n",
        "show(\"불용어 제거 전\", tokens)\n",
        "show(\"불용어 제거 후\", filtered)\n",
        "show(\"표제어 추출 후\", lemmatized)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) 정규표현식 핵심 패턴\n",
        "- 숫자 제거: `re.sub(r\"\\d+\", \"\", text)`\n",
        "- 다중 공백 정리: `re.sub(r\"\\s+\", \" \", text).strip()`\n",
        "- URL/이메일 제거 예시\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sample = \"Contact me at test@example.com and visit https://example.com, price 12345.\"\n",
        "no_digits = re.sub(r\"\\d+\", \"\", sample)\n",
        "no_urls = re.sub(r\"https?://\\S+\", \"\", no_digits)\n",
        "no_emails = re.sub(r\"\\S+@\\S+\", \"\", no_urls)\n",
        "normalized_space = re.sub(r\"\\s+\", \" \", no_emails).strip()\n",
        "\n",
        "show(\"원문\", sample)\n",
        "show(\"숫자 제거\", no_digits)\n",
        "show(\"URL 제거\", no_urls)\n",
        "show(\"이메일 제거\", no_emails)\n",
        "show(\"공백 정리\", normalized_space)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) 정수 인코딩(Integer Encoding)\n",
        "- 소규모 예시로 토큰→정수 매핑을 직접 구현\n",
        "- 실제 프로젝트에선 `Tokenizer`(Keras) 등 사용 가능\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "# 간단한 코퍼스\n",
        "docs = [\n",
        "    \"the quick brown fox\",\n",
        "    \"jumped over the lazy dog\",\n",
        "    \"the fox is quick and the dog is lazy\"\n",
        "]\n",
        "\n",
        "# 토큰화 및 어휘 집합\n",
        "tokenized = [d.split() for d in docs]\n",
        "vocab = Counter([t for doc in tokenized for t in doc])\n",
        "# 빈도 순 정렬 후 인덱스 부여\n",
        "vocab_sorted = [w for w, _ in vocab.most_common()]\n",
        "word2idx = {w: i+1 for i, w in enumerate(vocab_sorted)}  # 0은 패딩 예약\n",
        "\n",
        "encoded_docs = [[word2idx[t] for t in doc] for doc in tokenized]\n",
        "\n",
        "show(\"어휘 크기\", len(word2idx))\n",
        "show(\"word2idx\", word2idx)\n",
        "show(\"정수 인코딩 결과\", encoded_docs)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7) 시퀀스 패딩(Padding)\n",
        "- 최대 길이 혹은 분위수(예: 95%) 길이로 패딩 길이 결정\n",
        "- 앞/뒤 패딩 전략은 모델/데이터에 맞게 선택\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def pad_sequences(sequences, maxlen, value=0, padding='post'):\n",
        "    result = []\n",
        "    for seq in sequences:\n",
        "        if len(seq) >= maxlen:\n",
        "            if padding == 'post':\n",
        "                result.append(seq[:maxlen])\n",
        "            else:\n",
        "                result.append(seq[-maxlen:])\n",
        "        else:\n",
        "            if padding == 'post':\n",
        "                result.append(seq + [value] * (maxlen - len(seq)))\n",
        "            else:\n",
        "                result.append([value] * (maxlen - len(seq)) + seq)\n",
        "    return result\n",
        "\n",
        "maxlen = 6\n",
        "padded = pad_sequences(encoded_docs, maxlen=maxlen, value=0, padding='post')\n",
        "show(\"패딩 결과\", padded)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8) 원-핫 인코딩(One-Hot Encoding)\n",
        "- 분류 레이블 혹은 단어 인덱스를 원-핫 벡터로 표현\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def to_one_hot(indices, depth):\n",
        "    arr = np.zeros((len(indices), depth), dtype=int)\n",
        "    for i, idx in enumerate(indices):\n",
        "        if 0 <= idx < depth:\n",
        "            arr[i, idx] = 1\n",
        "    return arr\n",
        "\n",
        "sample_indices = [1, 3, 2]\n",
        "depth = 5\n",
        "show(\"원-핫 결과\", to_one_hot(sample_indices, depth))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9) 워드클라우드(Word Cloud)\n",
        "- 시각화 전처리: 불용어/소문자/구두점 제거 후 단어 빈도 계산\n",
        "- 시스템에 한글 폰트 경로가 있는 경우 설정 필요(예시는 영문)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "text_for_wc = \" \".join(lemmatized)\n",
        "wc = WordCloud(width=600, height=300, background_color='white').generate(text_for_wc)\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.imshow(wc, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10) 체크리스트 & 실전 팁\n",
        "- 전처리 정책은 학습/평가/예측에 동일 적용(함수화 권장)\n",
        "- 불용어는 도메인 맞춤 확장 리스트 유지\n",
        "- 길이 분포를 보고 패딩 길이 결정(과/미패딩 방지)\n",
        "- OOV 정책 미리 정의(특수 토큰, 최소 빈도 컷)\n",
        "- n-gram 범위, 토큰 케이스/구두점 정책은 검증으로 선택\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11) 복습 퀴즈 & 미니 과제\n",
        "- 퀴즈\n",
        "  1) Stemming과 Lemmatization의 차이를 한 문장으로 설명하세요.\n",
        "  2) OOV를 줄이는 방법 2가지를 적어보세요.\n",
        "  3) 패딩 길이를 정할 때 확인해야 할 분포는 무엇인가요?\n",
        "- 미니 과제\n",
        "  - 같은 말뭉치에 대해 (a) 구두점 제거 유무, (b) 불용어 리스트 변화, (c) n-gram 범위(1, 1-2) 조합을 바꿔 TF-IDF 스코어 상위 단어와 간단 분류 성능의 변화를 비교해보세요.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12) 한국어 불용어/형태소 기반 간단 예시\n",
        "- `KoNLPy`의 `Okt` 형태소기를 사용하여 한국어 토큰화/품사 기반 정규화 예시\n",
        "- 불용어: 제공된 `ko_stopwords.txt`를 로드하여 적용(도메인에 맞게 확장 권장)\n",
        "- 주의: Windows 환경에서는 `konlpy` 설치에 자바 런타임이 필요할 수 있음\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 필요 시 설치: pip install konlpy\n",
        "from konlpy.tag import Okt\n",
        "\n",
        "okt = Okt()\n",
        "\n",
        "korean_text = \"이 영화 진짜 재밌다ㅋㅋ 배우 연기가 너무 좋고 연출도 훌륭해요!\"\n",
        "\n",
        "# 형태소 단위 토큰화 + 명사/동사/형용사 위주 추출\n",
        "tokens_pos = okt.pos(korean_text, norm=True, stem=True)\n",
        "keep_pos = {\"Noun\", \"Verb\", \"Adjective\"}\n",
        "filtered_by_pos = [w for w, p in tokens_pos if p in keep_pos]\n",
        "\n",
        "# 불용어 로드\n",
        "stop_path = \"ko_stopwords.txt\"\n",
        "try:\n",
        "    with open(stop_path, 'r', encoding='utf-8') as f:\n",
        "        stop_ko = set([line.strip() for line in f if line.strip()])\n",
        "except FileNotFoundError:\n",
        "    stop_ko = set()\n",
        "\n",
        "filtered_ko = [w for w in filtered_by_pos if w not in stop_ko]\n",
        "\n",
        "print(\"원문:\", korean_text)\n",
        "print(\"품사태깅:\", tokens_pos)\n",
        "print(\"품사필터:\", filtered_by_pos)\n",
        "print(\"불용어제거:\", filtered_ko)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13) TF-IDF 비교 실습\n",
        "- 전처리 옵션 조합에 따른 TF-IDF 상위 단어 비교\n",
        "- 옵션 예시: 구두점 제거 유무, 불용어 적용 유무, n-gram=(1) vs (1,2)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "import string\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "corpus = [\n",
        "    \"The Matrix is a groundbreaking sci-fi movie.\",\n",
        "    \"This movie has excellent visual effects and a strong plot.\",\n",
        "    \"Many consider The Matrix a classic with deep philosophical themes.\",\n",
        "]\n",
        "\n",
        "# 전처리 함수(옵션)\n",
        "def preprocess(text, remove_punc=True, to_lower=True, stop=None):\n",
        "    if to_lower:\n",
        "        text = text.lower()\n",
        "    if remove_punc:\n",
        "        # 모든 구두점 제거를 안전하게 처리\n",
        "        text = re.sub(\"[\" + re.escape(string.punctuation) + \"]\", \" \", text)\n",
        "    if stop:\n",
        "        tokens = text.split()\n",
        "        text = \" \".join([t for t in tokens if t not in stop])\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "\n",
        "stop_en = set([\"the\", \"a\", \"and\", \"is\", \"this\", \"with\"])\n",
        "\n",
        "settings = [\n",
        "    dict(name=\"A: no_punc, no_stop, uni\", remove_punc=False, stop=None, ngram=(1,1)),\n",
        "    dict(name=\"B: punc, stop, uni\", remove_punc=True, stop=stop_en, ngram=(1,1)),\n",
        "    dict(name=\"C: punc, stop, uni+bi\", remove_punc=True, stop=stop_en, ngram=(1,2)),\n",
        "]\n",
        "\n",
        "rows = []\n",
        "for s in settings:\n",
        "    processed = [preprocess(doc, remove_punc=s['remove_punc'], to_lower=True, stop=s['stop']) for doc in corpus]\n",
        "    vec = TfidfVectorizer(ngram_range=s['ngram'], max_features=15)\n",
        "    X = vec.fit_transform(processed)\n",
        "    features = vec.get_feature_names_out()\n",
        "    scores = X.toarray().sum(axis=0)\n",
        "    top = sorted(zip(features, scores), key=lambda x: x[1], reverse=True)[:8]\n",
        "    rows.append({\"설정\": s['name'], \"상위 단어\": \", \".join([w for w,_ in top])})\n",
        "\n",
        "pd.DataFrame(rows)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
