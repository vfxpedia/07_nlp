{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ì „ì²˜ë¦¬ í•µì‹¬ ìš”ì•½: ì´ë¡ ê³¼ ì‹¤ìŠµ\n",
        "\n",
        "ì´ ë…¸íŠ¸ë¶ì€ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ì˜ í•µì‹¬ ê°œë…ê³¼ ìµœì†Œ ì‹¤í–‰ ì˜ˆì œë¥¼ í•œ ê³³ì— ì •ë¦¬í•´, í•™ìŠµìê°€ ë°”ë¡œ ì‹¤í–‰í•˜ë©° ë³µìŠµí•  ìˆ˜ ìˆë„ë¡ êµ¬ì„±í–ˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "- ëª©ì : ê°œë… ê³ ì°©, ì‹¤í–‰ ì¤‘ì‹¬ ë³µìŠµ, ì´í›„ Subword ì„¹ì…˜ ì¶”ê°€ ì‹œ ê¸°ë°˜ ì •ë¦¬\n",
        "- ë°ì´í„°: ê°„ë‹¨í•œ ì˜ˆì œ í…ìŠ¤íŠ¸ ì¤‘ì‹¬ (ëŒ€ìš©ëŸ‰ íŒŒì¼ì€ ë¯¸ì‚¬ìš©)\n",
        "- ì‹¤í–‰ ë°©ì‹: ê° ì„¹ì…˜ì˜ ì½”ë“œ ì…€ì„ ìˆœì„œëŒ€ë¡œ ì‹¤í–‰\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ“š ëª©ì°¨\n",
        "1. í•µì‹¬ í‚¤ì›Œë“œ í•œëˆˆì— ë³´ê¸°\n",
        "2. ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ê°œìš”\n",
        "3. í…ìŠ¤íŠ¸ ì •ê·œí™”ì™€ í† í°í™”\n",
        "4. ë¶ˆìš©ì–´ ì œê±°\n",
        "5. ì •ê·œí‘œí˜„ì‹ í•µì‹¬ íŒ¨í„´\n",
        "6. ì •ìˆ˜ ì¸ì½”ë”©(Integer Encoding)\n",
        "7. ì‹œí€€ìŠ¤ íŒ¨ë”©(Padding)\n",
        "8. ì›-í•« ì¸ì½”ë”©(One-Hot Encoding)\n",
        "9. ì›Œë“œí´ë¼ìš°ë“œ(Word Cloud)\n",
        "10. ì²´í¬ë¦¬ìŠ¤íŠ¸ & ì‹¤ì „ íŒ\n",
        "11. ë³µìŠµ í€´ì¦ˆ & ë¯¸ë‹ˆ ê³¼ì œ\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) í•µì‹¬ í‚¤ì›Œë“œ í•œëˆˆì— ë³´ê¸°\n",
        "- **Token**: í…ìŠ¤íŠ¸ì˜ ê¸°ë³¸ ë‹¨ìœ„\n",
        "- **Normalization**: ëŒ€ì†Œë¬¸ì í†µì¼, ê³µë°±/íŠ¹ìˆ˜ë¬¸ì ì²˜ë¦¬ ë“± ê·œì¹™í™”\n",
        "- **Stopwords**: ë¶„ì„ì— ëœ ìœ ì˜ë¯¸í•œ ë‹¨ì–´ë“¤(ì˜ˆ: the, and, ì€/ëŠ”/ì´/ê°€)\n",
        "- **Stemming vs Lemmatization**: ì–´ê°„ ì¶”ì¶œ(í˜•íƒœë§Œ) vs í‘œì œì–´ ì¶”ì¶œ(ì‚¬ì „ê¸°ë°˜ ì˜ë¯¸)\n",
        "- **Regex(ì •ê·œí‘œí˜„ì‹)**: íŒ¨í„´ ê¸°ë°˜ ì¹˜í™˜/ì¶”ì¶œ\n",
        "- **Integer Encoding**: í† í°ì„ ì •ìˆ˜ë¡œ ë§¤í•‘\n",
        "- **Padding**: ì‹œí€€ìŠ¤ ê¸¸ì´ í‘œì¤€í™”\n",
        "- **One-Hot**: ì •ìˆ˜ ë ˆì´ë¸”/í† í°ì„ ì§„ë¦¬í‘œ í˜•íƒœì˜ ë²¡í„°ë¡œ í‘œí˜„\n",
        "- **OOV(Out-Of-Vocabulary)**: í•™ìŠµ ì‚¬ì „ì— ì—†ëŠ” ë‹¨ì–´ ì²˜ë¦¬ ì „ëµ\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ê°œìš”\n",
        "1. í…ìŠ¤íŠ¸ ì •ê·œí™”(ì†Œë¬¸ì/ê³µë°±/íŠ¹ìˆ˜ë¬¸ì/ìˆ«ì ì²˜ë¦¬)\n",
        "2. í† í°í™”\n",
        "3. ë¶ˆìš©ì–´ ì œê±°(ë„ë©”ì¸ ë§ì¶¤ í™•ì¥)\n",
        "4. í‘œì œì–´/ì–´ê°„ ì²˜ë¦¬(ì–¸ì–´/ë„ë©”ì¸/ëª¨ë¸ íŠ¹ì„±ì— ë§ì¶° ì„ íƒ)\n",
        "5. ì •ìˆ˜ ì¸ì½”ë”© â†’ íŒ¨ë”©\n",
        "6. í•„ìš” ì‹œ ì›-í•«/ì„ë² ë”© ì„ íƒ\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ê³µí†µ ì˜ˆì œ í…ìŠ¤íŠ¸ì™€ ìœ í‹¸ í•¨ìˆ˜\n",
        "sample_text = \"The Matrix is everywhere! It's all around us; here, even in this room.\"\n",
        "\n",
        "def show(title, obj):\n",
        "    print(f\"\\n[{title}]\\n{obj}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) í…ìŠ¤íŠ¸ ì •ê·œí™”ì™€ í† í°í™”\n",
        "- ëŒ€ì†Œë¬¸ì í†µì¼, êµ¬ë‘ì  ì œê±° ì—¬ë¶€ëŠ” íƒœìŠ¤í¬ì— ë§ê²Œ ê²°ì •\n",
        "- í† í°í™”ëŠ” ì–¸ì–´/ë„ë©”ì¸ë³„ í† í¬ë‚˜ì´ì € ì„ íƒ(ì˜ì–´ ì˜ˆì‹œ ì‚¬ìš©)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "text = sample_text\n",
        "lowered = text.lower()\n",
        "# êµ¬ë‘ì  ì œê±°\n",
        "punc_table = str.maketrans('', '', string.punctuation)\n",
        "no_punc = lowered.translate(punc_table)\n",
        "\n",
        "show(\"ì›ë¬¸\", text)\n",
        "show(\"ì†Œë¬¸ì\", lowered)\n",
        "show(\"êµ¬ë‘ì  ì œê±°\", no_punc)\n",
        "show(\"ë‹¨ì–´ í† í°í™”\", word_tokenize(no_punc))\n",
        "show(\"ë¬¸ì¥ í† í°í™”\", sent_tokenize(text))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) ë¶ˆìš©ì–´ ì œê±°\n",
        "- ì¼ë°˜ ë¶ˆìš©ì–´ + ë„ë©”ì¸ ì»¤ìŠ¤í…€ ë¶ˆìš©ì–´ í•¨ê»˜ ì‚¬ìš© ê¶Œì¥\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "\n",
        "# í•„ìš” ë¦¬ì†ŒìŠ¤ ë‹¤ìš´ë¡œë“œ(ìµœì´ˆ 1íšŒ)\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "custom_stopwords = {\"it's\", \"us\", \"this\"}\n",
        "all_stopwords = stop_words.union(custom_stopwords)\n",
        "\n",
        "tokens = word_tokenize(no_punc)\n",
        "filtered = [t for t in tokens if t not in all_stopwords]\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized = [lemmatizer.lemmatize(t, pos='v') for t in filtered]\n",
        "\n",
        "show(\"ë¶ˆìš©ì–´ ì œê±° ì „\", tokens)\n",
        "show(\"ë¶ˆìš©ì–´ ì œê±° í›„\", filtered)\n",
        "show(\"í‘œì œì–´ ì¶”ì¶œ í›„\", lemmatized)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) ì •ê·œí‘œí˜„ì‹ í•µì‹¬ íŒ¨í„´\n",
        "- ìˆ«ì ì œê±°: `re.sub(r\"\\d+\", \"\", text)`\n",
        "- ë‹¤ì¤‘ ê³µë°± ì •ë¦¬: `re.sub(r\"\\s+\", \" \", text).strip()`\n",
        "- URL/ì´ë©”ì¼ ì œê±° ì˜ˆì‹œ\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sample = \"Contact me at test@example.com and visit https://example.com, price 12345.\"\n",
        "no_digits = re.sub(r\"\\d+\", \"\", sample)\n",
        "no_urls = re.sub(r\"https?://\\S+\", \"\", no_digits)\n",
        "no_emails = re.sub(r\"\\S+@\\S+\", \"\", no_urls)\n",
        "normalized_space = re.sub(r\"\\s+\", \" \", no_emails).strip()\n",
        "\n",
        "show(\"ì›ë¬¸\", sample)\n",
        "show(\"ìˆ«ì ì œê±°\", no_digits)\n",
        "show(\"URL ì œê±°\", no_urls)\n",
        "show(\"ì´ë©”ì¼ ì œê±°\", no_emails)\n",
        "show(\"ê³µë°± ì •ë¦¬\", normalized_space)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) ì •ìˆ˜ ì¸ì½”ë”©(Integer Encoding)\n",
        "- ì†Œê·œëª¨ ì˜ˆì‹œë¡œ í† í°â†’ì •ìˆ˜ ë§¤í•‘ì„ ì§ì ‘ êµ¬í˜„\n",
        "- ì‹¤ì œ í”„ë¡œì íŠ¸ì—ì„  `Tokenizer`(Keras) ë“± ì‚¬ìš© ê°€ëŠ¥\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "# ê°„ë‹¨í•œ ì½”í¼ìŠ¤\n",
        "docs = [\n",
        "    \"the quick brown fox\",\n",
        "    \"jumped over the lazy dog\",\n",
        "    \"the fox is quick and the dog is lazy\"\n",
        "]\n",
        "\n",
        "# í† í°í™” ë° ì–´íœ˜ ì§‘í•©\n",
        "tokenized = [d.split() for d in docs]\n",
        "vocab = Counter([t for doc in tokenized for t in doc])\n",
        "# ë¹ˆë„ ìˆœ ì •ë ¬ í›„ ì¸ë±ìŠ¤ ë¶€ì—¬\n",
        "vocab_sorted = [w for w, _ in vocab.most_common()]\n",
        "word2idx = {w: i+1 for i, w in enumerate(vocab_sorted)}  # 0ì€ íŒ¨ë”© ì˜ˆì•½\n",
        "\n",
        "encoded_docs = [[word2idx[t] for t in doc] for doc in tokenized]\n",
        "\n",
        "show(\"ì–´íœ˜ í¬ê¸°\", len(word2idx))\n",
        "show(\"word2idx\", word2idx)\n",
        "show(\"ì •ìˆ˜ ì¸ì½”ë”© ê²°ê³¼\", encoded_docs)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7) ì‹œí€€ìŠ¤ íŒ¨ë”©(Padding)\n",
        "- ìµœëŒ€ ê¸¸ì´ í˜¹ì€ ë¶„ìœ„ìˆ˜(ì˜ˆ: 95%) ê¸¸ì´ë¡œ íŒ¨ë”© ê¸¸ì´ ê²°ì •\n",
        "- ì•/ë’¤ íŒ¨ë”© ì „ëµì€ ëª¨ë¸/ë°ì´í„°ì— ë§ê²Œ ì„ íƒ\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def pad_sequences(sequences, maxlen, value=0, padding='post'):\n",
        "    result = []\n",
        "    for seq in sequences:\n",
        "        if len(seq) >= maxlen:\n",
        "            if padding == 'post':\n",
        "                result.append(seq[:maxlen])\n",
        "            else:\n",
        "                result.append(seq[-maxlen:])\n",
        "        else:\n",
        "            if padding == 'post':\n",
        "                result.append(seq + [value] * (maxlen - len(seq)))\n",
        "            else:\n",
        "                result.append([value] * (maxlen - len(seq)) + seq)\n",
        "    return result\n",
        "\n",
        "maxlen = 6\n",
        "padded = pad_sequences(encoded_docs, maxlen=maxlen, value=0, padding='post')\n",
        "show(\"íŒ¨ë”© ê²°ê³¼\", padded)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8) ì›-í•« ì¸ì½”ë”©(One-Hot Encoding)\n",
        "- ë¶„ë¥˜ ë ˆì´ë¸” í˜¹ì€ ë‹¨ì–´ ì¸ë±ìŠ¤ë¥¼ ì›-í•« ë²¡í„°ë¡œ í‘œí˜„\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def to_one_hot(indices, depth):\n",
        "    arr = np.zeros((len(indices), depth), dtype=int)\n",
        "    for i, idx in enumerate(indices):\n",
        "        if 0 <= idx < depth:\n",
        "            arr[i, idx] = 1\n",
        "    return arr\n",
        "\n",
        "sample_indices = [1, 3, 2]\n",
        "depth = 5\n",
        "show(\"ì›-í•« ê²°ê³¼\", to_one_hot(sample_indices, depth))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9) ì›Œë“œí´ë¼ìš°ë“œ(Word Cloud)\n",
        "- ì‹œê°í™” ì „ì²˜ë¦¬: ë¶ˆìš©ì–´/ì†Œë¬¸ì/êµ¬ë‘ì  ì œê±° í›„ ë‹¨ì–´ ë¹ˆë„ ê³„ì‚°\n",
        "- ì‹œìŠ¤í…œì— í•œê¸€ í°íŠ¸ ê²½ë¡œê°€ ìˆëŠ” ê²½ìš° ì„¤ì • í•„ìš”(ì˜ˆì‹œëŠ” ì˜ë¬¸)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "text_for_wc = \" \".join(lemmatized)\n",
        "wc = WordCloud(width=600, height=300, background_color='white').generate(text_for_wc)\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.imshow(wc, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10) ì²´í¬ë¦¬ìŠ¤íŠ¸ & ì‹¤ì „ íŒ\n",
        "- ì „ì²˜ë¦¬ ì •ì±…ì€ í•™ìŠµ/í‰ê°€/ì˜ˆì¸¡ì— ë™ì¼ ì ìš©(í•¨ìˆ˜í™” ê¶Œì¥)\n",
        "- ë¶ˆìš©ì–´ëŠ” ë„ë©”ì¸ ë§ì¶¤ í™•ì¥ ë¦¬ìŠ¤íŠ¸ ìœ ì§€\n",
        "- ê¸¸ì´ ë¶„í¬ë¥¼ ë³´ê³  íŒ¨ë”© ê¸¸ì´ ê²°ì •(ê³¼/ë¯¸íŒ¨ë”© ë°©ì§€)\n",
        "- OOV ì •ì±… ë¯¸ë¦¬ ì •ì˜(íŠ¹ìˆ˜ í† í°, ìµœì†Œ ë¹ˆë„ ì»·)\n",
        "- n-gram ë²”ìœ„, í† í° ì¼€ì´ìŠ¤/êµ¬ë‘ì  ì •ì±…ì€ ê²€ì¦ìœ¼ë¡œ ì„ íƒ\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11) ë³µìŠµ í€´ì¦ˆ & ë¯¸ë‹ˆ ê³¼ì œ\n",
        "- í€´ì¦ˆ\n",
        "  1) Stemmingê³¼ Lemmatizationì˜ ì°¨ì´ë¥¼ í•œ ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…í•˜ì„¸ìš”.\n",
        "  2) OOVë¥¼ ì¤„ì´ëŠ” ë°©ë²• 2ê°€ì§€ë¥¼ ì ì–´ë³´ì„¸ìš”.\n",
        "  3) íŒ¨ë”© ê¸¸ì´ë¥¼ ì •í•  ë•Œ í™•ì¸í•´ì•¼ í•  ë¶„í¬ëŠ” ë¬´ì—‡ì¸ê°€ìš”?\n",
        "- ë¯¸ë‹ˆ ê³¼ì œ\n",
        "  - ê°™ì€ ë§ë­‰ì¹˜ì— ëŒ€í•´ (a) êµ¬ë‘ì  ì œê±° ìœ ë¬´, (b) ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸ ë³€í™”, (c) n-gram ë²”ìœ„(1, 1-2) ì¡°í•©ì„ ë°”ê¿” TF-IDF ìŠ¤ì½”ì–´ ìƒìœ„ ë‹¨ì–´ì™€ ê°„ë‹¨ ë¶„ë¥˜ ì„±ëŠ¥ì˜ ë³€í™”ë¥¼ ë¹„êµí•´ë³´ì„¸ìš”.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12) í•œêµ­ì–´ ë¶ˆìš©ì–´/í˜•íƒœì†Œ ê¸°ë°˜ ê°„ë‹¨ ì˜ˆì‹œ\n",
        "- `KoNLPy`ì˜ `Okt` í˜•íƒœì†Œê¸°ë¥¼ ì‚¬ìš©í•˜ì—¬ í•œêµ­ì–´ í† í°í™”/í’ˆì‚¬ ê¸°ë°˜ ì •ê·œí™” ì˜ˆì‹œ\n",
        "- ë¶ˆìš©ì–´: ì œê³µëœ `ko_stopwords.txt`ë¥¼ ë¡œë“œí•˜ì—¬ ì ìš©(ë„ë©”ì¸ì— ë§ê²Œ í™•ì¥ ê¶Œì¥)\n",
        "- ì£¼ì˜: Windows í™˜ê²½ì—ì„œëŠ” `konlpy` ì„¤ì¹˜ì— ìë°” ëŸ°íƒ€ì„ì´ í•„ìš”í•  ìˆ˜ ìˆìŒ\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# í•„ìš” ì‹œ ì„¤ì¹˜: pip install konlpy\n",
        "from konlpy.tag import Okt\n",
        "\n",
        "okt = Okt()\n",
        "\n",
        "korean_text = \"ì´ ì˜í™” ì§„ì§œ ì¬ë°Œë‹¤ã…‹ã…‹ ë°°ìš° ì—°ê¸°ê°€ ë„ˆë¬´ ì¢‹ê³  ì—°ì¶œë„ í›Œë¥­í•´ìš”!\"\n",
        "\n",
        "# í˜•íƒœì†Œ ë‹¨ìœ„ í† í°í™” + ëª…ì‚¬/ë™ì‚¬/í˜•ìš©ì‚¬ ìœ„ì£¼ ì¶”ì¶œ\n",
        "tokens_pos = okt.pos(korean_text, norm=True, stem=True)\n",
        "keep_pos = {\"Noun\", \"Verb\", \"Adjective\"}\n",
        "filtered_by_pos = [w for w, p in tokens_pos if p in keep_pos]\n",
        "\n",
        "# ë¶ˆìš©ì–´ ë¡œë“œ\n",
        "stop_path = \"ko_stopwords.txt\"\n",
        "try:\n",
        "    with open(stop_path, 'r', encoding='utf-8') as f:\n",
        "        stop_ko = set([line.strip() for line in f if line.strip()])\n",
        "except FileNotFoundError:\n",
        "    stop_ko = set()\n",
        "\n",
        "filtered_ko = [w for w in filtered_by_pos if w not in stop_ko]\n",
        "\n",
        "print(\"ì›ë¬¸:\", korean_text)\n",
        "print(\"í’ˆì‚¬íƒœê¹…:\", tokens_pos)\n",
        "print(\"í’ˆì‚¬í•„í„°:\", filtered_by_pos)\n",
        "print(\"ë¶ˆìš©ì–´ì œê±°:\", filtered_ko)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13) TF-IDF ë¹„êµ ì‹¤ìŠµ\n",
        "- ì „ì²˜ë¦¬ ì˜µì…˜ ì¡°í•©ì— ë”°ë¥¸ TF-IDF ìƒìœ„ ë‹¨ì–´ ë¹„êµ\n",
        "- ì˜µì…˜ ì˜ˆì‹œ: êµ¬ë‘ì  ì œê±° ìœ ë¬´, ë¶ˆìš©ì–´ ì ìš© ìœ ë¬´, n-gram=(1) vs (1,2)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "import string\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "corpus = [\n",
        "    \"The Matrix is a groundbreaking sci-fi movie.\",\n",
        "    \"This movie has excellent visual effects and a strong plot.\",\n",
        "    \"Many consider The Matrix a classic with deep philosophical themes.\",\n",
        "]\n",
        "\n",
        "# ì „ì²˜ë¦¬ í•¨ìˆ˜(ì˜µì…˜)\n",
        "def preprocess(text, remove_punc=True, to_lower=True, stop=None):\n",
        "    if to_lower:\n",
        "        text = text.lower()\n",
        "    if remove_punc:\n",
        "        # ëª¨ë“  êµ¬ë‘ì  ì œê±°ë¥¼ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬\n",
        "        text = re.sub(\"[\" + re.escape(string.punctuation) + \"]\", \" \", text)\n",
        "    if stop:\n",
        "        tokens = text.split()\n",
        "        text = \" \".join([t for t in tokens if t not in stop])\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "\n",
        "stop_en = set([\"the\", \"a\", \"and\", \"is\", \"this\", \"with\"])\n",
        "\n",
        "settings = [\n",
        "    dict(name=\"A: no_punc, no_stop, uni\", remove_punc=False, stop=None, ngram=(1,1)),\n",
        "    dict(name=\"B: punc, stop, uni\", remove_punc=True, stop=stop_en, ngram=(1,1)),\n",
        "    dict(name=\"C: punc, stop, uni+bi\", remove_punc=True, stop=stop_en, ngram=(1,2)),\n",
        "]\n",
        "\n",
        "rows = []\n",
        "for s in settings:\n",
        "    processed = [preprocess(doc, remove_punc=s['remove_punc'], to_lower=True, stop=s['stop']) for doc in corpus]\n",
        "    vec = TfidfVectorizer(ngram_range=s['ngram'], max_features=15)\n",
        "    X = vec.fit_transform(processed)\n",
        "    features = vec.get_feature_names_out()\n",
        "    scores = X.toarray().sum(axis=0)\n",
        "    top = sorted(zip(features, scores), key=lambda x: x[1], reverse=True)[:8]\n",
        "    rows.append({\"ì„¤ì •\": s['name'], \"ìƒìœ„ ë‹¨ì–´\": \", \".join([w for w,_ in top])})\n",
        "\n",
        "pd.DataFrame(rows)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
