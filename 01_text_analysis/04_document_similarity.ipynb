{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d0c3d70",
   "metadata": {},
   "source": [
    "# 문서 유사도 (Document Similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4fd136d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97628461",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob\n",
    "import pandas as pd\n",
    "\n",
    "path = 'C:\\\\SKN_19\\\\07_nlp\\\\01_text_analysis\\\\data\\\\OpinosisDataset1.0\\\\topics'\n",
    "\n",
    "all_files = glob.glob(os.path.join(path, '*.data'))\n",
    "\n",
    "filename_list = []\n",
    "opinion_list = []\n",
    "\n",
    "for file_ in all_files:\n",
    "    df = pd.read_table(file_, header=None, index_col=None, encoding='latin1')\n",
    "    # \"C:\\\\SKN_19\\\\07_nlp\\\\01_text_analysis\\\\data\\\\OpinosisDataset1.0\\\\topics\\\\accuracy_garmin_nuvi_255W_gps.txt.data\"\n",
    "\n",
    "    filename = file_.split('\\\\')[-1]\n",
    "    filename = filename.split('.')[0]\n",
    "    filename_list.append(filename)\n",
    "\n",
    "    opinions = df.to_string(index=False, header=False)\n",
    "    opinion_list.append(opinions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289b0803",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_df = pd.DataFrame({\n",
    "    'filename': filename_list,\n",
    "    'opinions': opinion_list \n",
    "})\n",
    "\n",
    "document_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74a3ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "\n",
    "def lemmatize(text):\n",
    "    \"\"\"\n",
    "    문자열을 소문자로 변환하고, 문장 부호를 제거한 후\n",
    "    토큰화하여 각 단어의 동사(verb) 형태의 어근(lemmatize)으로 변환합니다.\n",
    "    \n",
    "    주요 처리 단계:\n",
    "    1. text.lower(): 입력 문자열 전체를 소문자로 변환합니다.\n",
    "    2. punc_rem_dict: string.punctuation에 포함된 모든 문장 부호 문자에 대해\n",
    "       ord 함수(문자를 유니코드 정수값으로 변환)를 사용해 딕셔너리를 만듭니다.\n",
    "       (예: ord('!') → 33, ord('.') → 46 등)\n",
    "       이런 식으로 만들어진 딕셔너리는 translate 메서드에서 각 문장 부호 문자를 모두 삭제하는 역할을 합니다.\n",
    "    3. text.translate(punc_rem_dict): 위 딕셔너리로 text에서 해당 문자 제거.\n",
    "    4. nltk.word_tokenize(text): 텍스트를 단어 단위로 분리(토큰화)합니다.\n",
    "    5. WordNetLemmatizer().lemmatize(token, pos='v'): 각 토큰을 동사로 간주하여 어근 추출.\n",
    "    \n",
    "    최종적으로 어근화된 단어 리스트를 반환합니다.\n",
    "    \"\"\"\n",
    "    # 1. 소문자 변환\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 2. 문장 부호 제거: ord(ch)는 문자를 아스키코드(또는 유니코드) 정수로 변환\n",
    "    punc_rem_dict = dict((ord(ch), None) for ch in punctuation)\n",
    "    text = text.translate(punc_rem_dict)\n",
    "\n",
    "    # 3. 단어 토큰화\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "\n",
    "    # 4. 어근화(lemmatization)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(token, pos='v') for token in tokens]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
