{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# NLP í•µì‹¬ ìš”ì•½: ì´ë¡ ê³¼ ì‹¤ìŠµ\n",
        "\n",
        "ì´ ë…¸íŠ¸ë¶ì€ ìì—°ì–´ ì²˜ë¦¬(NLP)ì˜ í•µì‹¬ ê°œë…ê³¼ ì‹¤ìŠµì„ ì²´ê³„ì ìœ¼ë¡œ ì •ë¦¬í•œ ìë£Œì…ë‹ˆë‹¤.\n",
        "ì•ìœ¼ë¡œì˜ ê³µë¶€ì— ë„ì›€ì´ ë˜ë„ë¡ ì´ë¡ ê³¼ ì‹¤ìŠµ ì½”ë“œë¥¼ í•¨ê»˜ ì œê³µí•©ë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ“š ëª©ì°¨\n",
        "1. [NLP ê°œìš”](#nlp-ê°œìš”)\n",
        "2. [í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬](#í…ìŠ¤íŠ¸-ì „ì²˜ë¦¬)\n",
        "3. [íŠ¹ì„± ë²¡í„°í™”](#íŠ¹ì„±-ë²¡í„°í™”)\n",
        "4. [ë¬¸ì„œ êµ°ì§‘í™”](#ë¬¸ì„œ-êµ°ì§‘í™”)\n",
        "5. [ë¬¸ì„œ ìœ ì‚¬ë„](#ë¬¸ì„œ-ìœ ì‚¬ë„)\n",
        "6. [í•µì‹¬ í‚¤ì›Œë“œ ì •ë¦¬](#í•µì‹¬-í‚¤ì›Œë“œ-ì •ë¦¬)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 1. NLP ê°œìš”\n",
        "\n",
        "### ğŸ¯ í•µì‹¬ ê°œë…\n",
        "\n",
        "**ìì—°ì–´ ì²˜ë¦¬(NLP)** ë€ ì»´í“¨í„°ê°€ ì¸ê°„ì˜ ì–¸ì–´ë¥¼ ì´í•´í•˜ê³ , ìƒì„±í•˜ë©° í™œìš©í•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤.\n",
        "\n",
        "### ğŸ“Š NLP ê¸°ìˆ  ì ìš© ë¶„ì•¼\n",
        "\n",
        "#### 1. ìì—°ì–´ ì´í•´\n",
        "- **ê°ì„±ë¶„ì„** (Sentiment Analysis): ë¦¬ë·°ê°€ ê¸ì •ì¸ê°€ ë¶€ì •ì¸ê°€\n",
        "- **ì£¼ì œë¶„ë¥˜** (Topic Modeling): ë‰´ìŠ¤ ë‚´ìš©ì„ ì •ì¹˜/ìŠ¤í¬ì¸ /ê³¼í•™ ë“±ìœ¼ë¡œ ë¶„ë¥˜\n",
        "- **í˜•íƒœì†Œ ë¶„ì„**: ë‹¨ì–´ì˜ í˜•íƒœì†Œ ë‹¨ìœ„ë¡œ ë¶„í•´\n",
        "- **ê°œì²´ëª…ì¸ì‹** (NER): ì‚¬ëŒì´ë¦„/ì§€ì—­ ì´ë¦„ ë“±ì„ êµ¬ë¶„\n",
        "- **ì² ìë²• êµì •** (Spelling Correction)\n",
        "- **ìŠ¤íŒ¸ íƒì§€** (Spam Detection)\n",
        "\n",
        "#### 2. ìì—°ì–´ ìƒì„±\n",
        "- **ë¬¸ì¥ ìƒì„±** (Text Generation)\n",
        "- **ë²ˆì—­** (Text Translation)\n",
        "- **ëŒ€í™”** (Chatbot)\n",
        "- **ìš”ì•½** (Text Summarization)\n",
        "- **ì´ë¯¸ì§€/ë¹„ë””ì˜¤ ì„¤ëª…** (Image/Video Captioning)\n",
        "\n",
        "### ğŸ”„ NLP ë°œì „ ë‹¨ê³„\n",
        "\n",
        "| **êµ¬ë¶„** | **ê¸°ì¡´ ë°©ì‹ (2010ë…„ëŒ€ ì´ì „)** | **ìµœê·¼ ë°©ì‹ (ë”¥ëŸ¬ë‹ ê¸°ë°˜)** |\n",
        "|----------|------------------------------|---------------------------|\n",
        "| **íŠ¹ì§•** | ê·œì¹™ê¸°ë°˜, í†µê³„ê¸°ë°˜, ì „í†µì  ML | Deep Learning, Word Embedding |\n",
        "| **ì ‘ê·¼ë²•** | ì–¸ì–´í•™ ê¸°ë°˜ rule-based | ë¬¸ë§¥, ì˜ë¯¸, ì–´ìˆœ ìë™ í•™ìŠµ |\n",
        "| **ê¸°ìˆ ** | Corpus ê¸°ë°˜ í†µê³„ ëª¨ë¸ | Word2Vec, BERT, GPT, Transformer |\n",
        "\n",
        "### ğŸ“ í•µì‹¬ ìš©ì–´\n",
        "\n",
        "| ìš©ì–´ | ì„¤ëª… | ì˜ˆì‹œ |\n",
        "|------|------|------|\n",
        "| **ë§ë­‰ì¹˜(Corpus)** | í…ìŠ¤íŠ¸ ë°ì´í„°ì˜ ëª¨ìŒ | ë‰´ìŠ¤ ê¸°ì‚¬ ëª¨ìŒ, ë¦¬ë·° ë°ì´í„° |\n",
        "| **í† í°(Token)** | í…ìŠ¤íŠ¸ì˜ ê¸°ë³¸ ë‹¨ìœ„ | \"I love NLP!\" â†’ [\"I\", \"love\", \"NLP\", \"!\"] |\n",
        "| **ë¬¸ì¥(Sentence)** | ë¬¸ë§¥ì˜ ë‹¨ìœ„ | êµ¬ë‘ì ìœ¼ë¡œ êµ¬ë¶„ëœ í…ìŠ¤íŠ¸ |\n",
        "| **ë‹¨ì–´(Word)** | ì˜ë¯¸ë¥¼ ê°€ì§€ëŠ” ìµœì†Œ ë‹¨ìœ„ | \"I\", \"love\", \"NLP\" |\n",
        "| **í˜•íƒœì†Œ(Morpheme)** | ì˜ë¯¸ì˜ ìµœì†Œ ë‹¨ìœ„ | \"unbreakable\" â†’ [\"un-\", \"break\", \"-able\"] |\n",
        "| **ë¶ˆìš©ì–´(Stopwords)** | ì˜ë¯¸ ë¶„ì„ì— ë„ì›€ ì•ˆë˜ëŠ” ë‹¨ì–´ | \"the\", \"and\", \"is\", \"ì€\", \"ëŠ”\" |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 2. í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬\n",
        "\n",
        "### ğŸ› ï¸ ì „ì²˜ë¦¬ ë‹¨ê³„\n",
        "\n",
        "1. **í…ìŠ¤íŠ¸ ì •ê·œí™”** (Text Normalization)\n",
        "2. **í† í°í™”** (Tokenization)\n",
        "3. **ë¶ˆìš©ì–´ ì œê±°** (Stopword Removal)\n",
        "4. **ì–´ê°„ ì¶”ì¶œ/í‘œì œì–´ ì¶”ì¶œ** (Stemming/Lemmatization)\n",
        "\n",
        "### ğŸ’» ì‹¤ìŠµ ì½”ë“œ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ import\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# NLTK ë°ì´í„° ë‹¤ìš´ë¡œë“œ\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "print(\"ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì™„ë£Œ!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1 í† í°í™” (Tokenization)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ì˜ˆì œ í…ìŠ¤íŠ¸\n",
        "text = 'NLTK is a powerful library for NLP!!!!! I love it.'\n",
        "\n",
        "# ë‹¨ì–´ í† í°í™”\n",
        "word_tokens = word_tokenize(text)\n",
        "print(\"ë‹¨ì–´ í† í°í™”:\", word_tokens)\n",
        "\n",
        "# ë¬¸ì¥ í† í°í™”\n",
        "sent_tokens = sent_tokenize(text)\n",
        "print(\"ë¬¸ì¥ í† í°í™”:\", sent_tokens)\n",
        "\n",
        "# N-gram ìƒì„±\n",
        "from nltk import ngrams\n",
        "bigrams = list(ngrams(word_tokens, 2))\n",
        "print(\"Bigram:\", bigrams[:5])  # ì²˜ìŒ 5ê°œë§Œ ì¶œë ¥\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 ê°ì„±ë¶„ì„ (Sentiment Analysis)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# VADER ê°ì„±ë¶„ì„ê¸° ì´ˆê¸°í™”\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "# ê°ì„±ë¶„ì„ ì˜ˆì œ\n",
        "texts = [\n",
        "    'I absolutely love this product!',\n",
        "    'This is okay, I guess',\n",
        "    'I hate it so much',\n",
        "    'The weather is nice today'\n",
        "]\n",
        "\n",
        "for text in texts:\n",
        "    scores = analyzer.polarity_scores(text)\n",
        "    print(f'\\ní…ìŠ¤íŠ¸: {text}')\n",
        "    print(f'ì ìˆ˜: {scores}')\n",
        "    print(f'ê²°ê³¼: {\"ê¸ì •\" if scores[\"compound\"] > 0.1 else \"ë¶€ì •\" if scores[\"compound\"] < -0.1 else \"ì¤‘ë¦½\"}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.3 ë¶ˆìš©ì–´ ì œê±° ë° ì •ê·œí™”\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ í•¨ìˆ˜\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ í•¨ìˆ˜\n",
        "    1. ì†Œë¬¸ì ë³€í™˜\n",
        "    2. ë¬¸ì¥ ë¶€í˜¸ ì œê±°\n",
        "    3. í† í°í™”\n",
        "    4. ë¶ˆìš©ì–´ ì œê±°\n",
        "    5. ì–´ê·¼ ì¶”ì¶œ\n",
        "    \"\"\"\n",
        "    # 1. ì†Œë¬¸ì ë³€í™˜\n",
        "    text = text.lower()\n",
        "    \n",
        "    # 2. ë¬¸ì¥ ë¶€í˜¸ ì œê±°\n",
        "    punc_rem_dict = dict((ord(ch), None) for ch in string.punctuation)\n",
        "    text = text.translate(punc_rem_dict)\n",
        "    \n",
        "    # 3. í† í°í™”\n",
        "    tokens = word_tokenize(text)\n",
        "    \n",
        "    # 4. ë¶ˆìš©ì–´ ì œê±°\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "    \n",
        "    # 5. ì–´ê·¼ ì¶”ì¶œ\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(token, pos='v') for token in tokens]\n",
        "    \n",
        "    return tokens\n",
        "\n",
        "# ì „ì²˜ë¦¬ í…ŒìŠ¤íŠ¸\n",
        "sample_text = 'The Matrix is everywhere its all around us, here even in this room!!!!!'\n",
        "processed = preprocess_text(sample_text)\n",
        "print(\"ì›ë³¸:\", sample_text)\n",
        "print(\"ì „ì²˜ë¦¬ í›„:\", processed)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 3. íŠ¹ì„± ë²¡í„°í™”\n",
        "\n",
        "### ğŸ“Š BOW vs Word Embedding\n",
        "\n",
        "| **êµ¬ë¶„** | **Bag-of-Words (BOW)** | **Word Embedding** |\n",
        "|----------|------------------------|-------------------|\n",
        "| **ê°œë…** | ë‹¨ì–´ ì¶œí˜„ ë¹ˆë„ë¡œ ë¬¸ì„œ í‘œí˜„ | ë‹¨ì–´ë¥¼ ì‹¤ìˆ˜ ë²¡í„°ë¡œ í‘œí˜„ |\n",
        "| **íŠ¹ì§•** | ìˆœì„œ/ì˜ë¯¸ ë¬´ì‹œ, ê³ ì°¨ì› í¬ì†Œ ë²¡í„° | ì˜ë¯¸ì  ìœ ì‚¬ì„± ë°˜ì˜, ë°€ì§‘ ì €ì°¨ì› ë²¡í„° |\n",
        "| **ë°©ë²•** | CountVectorizer, TF-IDF | Word2Vec, GloVe, FastText |\n",
        "| **ì¥ì ** | êµ¬í˜„ ê°„ë‹¨, ì´í•´ ì‰¬ì›€ | ë¬¸ë§¥ ì •ë³´ ë°˜ì˜, ìœ ì‚¬ ë‹¨ì–´ ê·¼ì ‘ ë°°ì¹˜ |\n",
        "| **ë‹¨ì ** | ì˜ë¯¸ ê´€ê³„/ìˆœì„œ ì •ë³´ ì—†ìŒ | ë§ì€ ë°ì´í„°/í•™ìŠµ ì‹œê°„ í•„ìš” |\n",
        "\n",
        "### ğŸ’» CountVectorizer ì‹¤ìŠµ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ì˜ˆì œ ë¬¸ì„œë“¤\n",
        "documents = [\n",
        "    'The Matrix is everywhere its all around us, here even in this room.',\n",
        "    'You take the blue pill and the story ends.',\n",
        "    'You take the red pill and you stay in Wonderland.'\n",
        "]\n",
        "\n",
        "# CountVectorizer ì‚¬ìš©\n",
        "count_vectorizer = CountVectorizer(\n",
        "    stop_words='english',  # ë¶ˆìš©ì–´ ì œê±°\n",
        "    ngram_range=(1, 2),    # 1-gramê³¼ 2-gram ì‚¬ìš©\n",
        "    max_features=20        # ìƒìœ„ 20ê°œ íŠ¹ì„±ë§Œ ì‚¬ìš©\n",
        ")\n",
        "\n",
        "# ë²¡í„°í™”\n",
        "count_matrix = count_vectorizer.fit_transform(documents)\n",
        "print(\"ë¬¸ì„œ-íŠ¹ì„± í–‰ë ¬ í˜•íƒœ:\", count_matrix.shape)\n",
        "print(\"\\níŠ¹ì„± ì´ë¦„ë“¤:\")\n",
        "print(count_vectorizer.get_feature_names_out())\n",
        "print(\"\\në²¡í„°í™” ê²°ê³¼:\")\n",
        "print(count_matrix.toarray())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ğŸ“ˆ TF-IDF ì‹¤ìŠµ\n",
        "\n",
        "**TF-IDF ê³µì‹:**\n",
        "- **TF (Term Frequency)**: tf(t,d) = ë‹¨ì–´ tì˜ ë¬¸ì„œ d ë‚´ ë“±ì¥ íšŸìˆ˜ / ë¬¸ì„œ dì˜ ì „ì²´ ë‹¨ì–´ ìˆ˜\n",
        "- **IDF (Inverse Document Frequency)**: idf(t) = log(N / (1 + df(t)))\n",
        "- **TF-IDF**: tf-idf(t,d) = tf(t,d) Ã— idf(t)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TfidfVectorizer ì‚¬ìš©\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1, 2), max_features=20)\n",
        "\n",
        "# TF-IDF ë²¡í„°í™”\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
        "print(\"TF-IDF í–‰ë ¬ í˜•íƒœ:\", tfidf_matrix.shape)\n",
        "print(\"\\nTF-IDF ì ìˆ˜:\")\n",
        "df_tfidf = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out(), index=[f'ë¬¸ì„œ{i+1}' for i in range(len(documents))])\n",
        "print(df_tfidf.round(3))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 4. ë¬¸ì„œ êµ°ì§‘í™” & ìœ ì‚¬ë„\n",
        "\n",
        "### ğŸ¯ K-Means í´ëŸ¬ìŠ¤í„°ë§ê³¼ ì½”ì‚¬ì¸ ìœ ì‚¬ë„\n",
        "\n",
        "**í•µì‹¬ ê°œë…:**\n",
        "- **K-Means**: ìœ ì‚¬í•œ ë¬¸ì„œë“¤ì„ Kê°œì˜ ê·¸ë£¹ìœ¼ë¡œ ë¶„ë¥˜\n",
        "- **ì½”ì‚¬ì¸ ìœ ì‚¬ë„**: ë‘ ë²¡í„° ê°„ ê°ë„ë¡œ ìœ ì‚¬ì„± ì¸¡ì • (1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ìœ ì‚¬)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ìƒ˜í”Œ ë¬¸ì„œ ìƒì„±\n",
        "sample_docs = [\n",
        "    'The GPS navigation system is very accurate and reliable.',\n",
        "    'Hotel rooms are clean and comfortable with great service.',\n",
        "    'The car battery life is excellent for long trips.',\n",
        "    'Restaurant food quality is outstanding with fresh ingredients.',\n",
        "    'The screen display is clear and bright for reading.',\n",
        "    'Staff members are friendly and helpful at all times.'\n",
        "]\n",
        "\n",
        "# TF-IDF ë²¡í„°í™”\n",
        "tfidf_vec = TfidfVectorizer(stop_words='english', ngram_range=(1, 2), max_df=0.9, min_df=0.1)\n",
        "doc_vecs = tfidf_vec.fit_transform(sample_docs)\n",
        "\n",
        "# K-Means í´ëŸ¬ìŠ¤í„°ë§\n",
        "kmeans = KMeans(n_clusters=3, random_state=0)\n",
        "clusters = kmeans.fit_predict(doc_vecs)\n",
        "\n",
        "# ê²°ê³¼ ì¶œë ¥\n",
        "doc_df = pd.DataFrame({'ë¬¸ì„œ': sample_docs, 'í´ëŸ¬ìŠ¤í„°': clusters})\n",
        "print(\"í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼:\")\n",
        "print(doc_df)\n",
        "\n",
        "# ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°\n",
        "cosine_sim = cosine_similarity(doc_vecs)\n",
        "print(\"\\nì½”ì‚¬ì¸ ìœ ì‚¬ë„ í–‰ë ¬:\")\n",
        "sim_df = pd.DataFrame(cosine_sim, index=[f'ë¬¸ì„œ{i+1}' for i in range(len(sample_docs))], columns=[f'ë¬¸ì„œ{i+1}' for i in range(len(sample_docs))])\n",
        "print(sim_df.round(3))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 5. í•µì‹¬ í‚¤ì›Œë“œ ì •ë¦¬\n",
        "\n",
        "### ğŸ”‘ ë°˜ë“œì‹œ ê¸°ì–µí•´ì•¼ í•  í‚¤ì›Œë“œ\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **1. ê¸°ë³¸ ê°œë…**\n",
        "- **Corpus (ë§ë­‰ì¹˜)**: ë¶„ì„í•  í…ìŠ¤íŠ¸ ë°ì´í„° ëª¨ìŒ\n",
        "- **Token (í† í°)**: í…ìŠ¤íŠ¸ì˜ ê¸°ë³¸ ë¶„ì„ ë‹¨ìœ„\n",
        "- **Stopwords (ë¶ˆìš©ì–´)**: 'the', 'is', 'and' ë“± ì˜ë¯¸ ì—†ëŠ” ë‹¨ì–´\n",
        "- **Lemmatization (í‘œì œì–´ ì¶”ì¶œ)**: ë‹¨ì–´ì˜ ê¸°ë³¸í˜• ì¶”ì¶œ (running â†’ run)\n",
        "\n",
        "#### **2. ë²¡í„°í™” ë°©ë²•**\n",
        "- **CountVectorizer**: ë‹¨ì–´ ì¶œí˜„ íšŸìˆ˜ ê¸°ë°˜ ë²¡í„°í™”\n",
        "- **TF-IDF**: ë‹¨ì–´ì˜ ì¤‘ìš”ë„ ê°€ì¤‘ì¹˜ ê³„ì‚°\n",
        "  - **TF** (Term Frequency): ë¬¸ì„œ ë‚´ ë‹¨ì–´ ë¹ˆë„\n",
        "  - **IDF** (Inverse Document Frequency): ì „ì²´ ë¬¸ì„œì—ì„œì˜ í¬ì†Œì„±\n",
        "- **N-gram**: ì—°ì†ëœ Nê°œ ë‹¨ì–´ ì¡°í•© (bigram, trigram ë“±)\n",
        "\n",
        "#### **3. ë¶„ì„ ê¸°ë²•**\n",
        "- **Sentiment Analysis (ê°ì„±ë¶„ì„)**: í…ìŠ¤íŠ¸ì˜ ê¸ì •/ë¶€ì •/ì¤‘ë¦½ íŒë‹¨\n",
        "- **K-Means Clustering**: ìœ ì‚¬ ë¬¸ì„œ ê·¸ë£¹í™”\n",
        "- **Cosine Similarity**: ë¬¸ì„œ ê°„ ìœ ì‚¬ë„ ì¸¡ì • (0~1 ì‚¬ì´ ê°’)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ğŸ“š ì£¼ìš” í•¨ìˆ˜ ì¹˜íŠ¸ì‹œíŠ¸\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ì£¼ìš” í•¨ìˆ˜ ì‚¬ìš©ë²• ì •ë¦¬\n",
        "\n",
        "# 1. í† í°í™”\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "word_tokenize(\"Hello world!\")  # ['Hello', 'world', '!']\n",
        "sent_tokenize(\"Hi. How are you?\")  # ['Hi.', 'How are you?']\n",
        "\n",
        "# 2. ë¶ˆìš©ì–´\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# 3. í‘œì œì–´ ì¶”ì¶œ\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatizer.lemmatize('running', pos='v')  # 'run'\n",
        "\n",
        "# 4. ê°ì„±ë¶„ì„\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "analyzer.polarity_scores(\"I love this!\")  # {'neg': 0.0, 'neu': 0.192, 'pos': 0.808, 'compound': 0.6369}\n",
        "\n",
        "# 5. CountVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cv = CountVectorizer(stop_words='english', ngram_range=(1,2), max_features=100)\n",
        "# cv.fit_transform(documents)\n",
        "\n",
        "# 6. TF-IDF\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf = TfidfVectorizer(stop_words='english', ngram_range=(1,2), max_df=0.85, min_df=0.05)\n",
        "# tfidf.fit_transform(documents)\n",
        "\n",
        "# 7. K-Means\n",
        "from sklearn.cluster import KMeans\n",
        "kmeans = KMeans(n_clusters=3, random_state=0)\n",
        "# kmeans.fit_predict(vectors)\n",
        "\n",
        "# 8. ì½”ì‚¬ì¸ ìœ ì‚¬ë„\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "# cosine_similarity(vec1, vec2)\n",
        "\n",
        "print(\"ì£¼ìš” í•¨ìˆ˜ ì‚¬ìš©ë²•ì„ ì •ë¦¬í–ˆìŠµë‹ˆë‹¤!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ğŸ¯ ì‹¤ë¬´ ì ìš© íŒ\n",
        "\n",
        "1. **ì „ì²˜ë¦¬ê°€ ì„±ëŠ¥ì˜ 80%ë¥¼ ê²°ì •**\n",
        "   - ì†Œë¬¸ì ë³€í™˜, íŠ¹ìˆ˜ë¬¸ì ì œê±°, ë¶ˆìš©ì–´ ì œê±° í•„ìˆ˜\n",
        "   - ë„ë©”ì¸ì— ë§ëŠ” ì»¤ìŠ¤í…€ ë¶ˆìš©ì–´ ì¶”ê°€\n",
        "\n",
        "2. **TF-IDF > CountVectorizer**\n",
        "   - ëŒ€ë¶€ë¶„ì˜ ê²½ìš° TF-IDFê°€ ë” ì¢‹ì€ ì„±ëŠ¥\n",
        "   - ë¬¸ì„œì˜ ì¤‘ìš”ë„ë¥¼ ì˜ ë°˜ì˜\n",
        "\n",
        "3. **N-gram í™œìš©**\n",
        "   - unigramë§Œ ì‚¬ìš©í•˜ë©´ ë¬¸ë§¥ ì†ì‹¤\n",
        "   - bigram, trigramìœ¼ë¡œ ì˜ë¯¸ ì¡°í•© ë³´ì¡´\n",
        "\n",
        "4. **ì ì ˆí•œ í´ëŸ¬ìŠ¤í„° ìˆ˜ ì„ íƒ**\n",
        "   - Elbow methodë¡œ ìµœì  Kê°’ ì°¾ê¸°\n",
        "   - ë„ë©”ì¸ ì§€ì‹ í™œìš©\n",
        "\n",
        "5. **ì „ì²˜ë¦¬ í•¨ìˆ˜ ì¬ì‚¬ìš©**\n",
        "   - í•™ìŠµ ë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ë™ì¼í•œ ì „ì²˜ë¦¬ ì ìš©\n",
        "   - í•¨ìˆ˜ë¡œ ë§Œë“¤ì–´ ì¬ì‚¬ìš©ì„± ë†’ì´ê¸°\n",
        "# e3dfdfdfdfdffã„´ã„´ã…‡ã„¹ã…‡ã…‡ã…‡ã…‡ã…‡ã…‡ã„¹text = \"Theater attendance has decreased. The THEATER industry is adapting.\"# ëŒ€ì†Œë¬¸ì í†µí•©\n",
        "text = \"Theater attendance has decreased. The THEATER industry is adapting.\"\n",
        "\n",
        "transformed_text = text.lower()\n",
        "transformed_text# ëŒ€ì†Œë¬¸ì í†µí•©\n",
        "text = \"Theater attendance has decreased. The THEATER industry is adapting.\"\n",
        "\n",
        "transformed_text = text.lower()\n",
        "transformed_text# ëŒ€ì†Œë¬¸ì í†µí•©\n",
        "text = \"Theater attendance has decreased. The THEATER industry is adapting.\"\n",
        "\n",
        "transformed_text = text.lower()\n",
        "transformed_text# ëŒ€ì†Œë¬¸ì í†µí•©\n",
        "text = \"Theater attendance has decreased. The THEATER industry is adapting.\"\n",
        "\n",
        "transformed_text = text.lower()\n",
        "transformed_textã…‡ã…‡ã…‡"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ğŸ“– ë‹¤ìŒ í•™ìŠµ ë‹¨ê³„\n",
        "\n",
        "#### **ì¤‘ê¸‰ ë‹¨ê³„**\n",
        "1. **Word Embedding**: Word2Vec, GloVe, FastText\n",
        "2. **í•œêµ­ì–´ NLP**: KoNLPy, í˜•íƒœì†Œ ë¶„ì„ê¸° (Mecab, Okt, Komoran)\n",
        "3. **ì‹¤ë¬´ ë„êµ¬**: spaCy, Gensim\n",
        "4. **Topic Modeling**: LDA, LSA\n",
        "\n",
        "#### **ê³ ê¸‰ ë‹¨ê³„**\n",
        "1. **ë”¥ëŸ¬ë‹ NLP**: RNN, LSTM, GRU\n",
        "2. **Attention & Transformer**: Self-Attention, Multi-Head Attention\n",
        "3. **ìµœì‹  ëª¨ë¸**: BERT, GPT, T5\n",
        "4. **Fine-tuning**: Pre-trained ëª¨ë¸ í™œìš©\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ’¡ ë§ˆë¬´ë¦¬\n",
        "\n",
        "ì´ ë…¸íŠ¸ë¶ì„ í†µí•´ NLPì˜ í•µì‹¬ ê°œë…ê³¼ ì‹¤ìŠµì„ ì²´ê³„ì ìœ¼ë¡œ í•™ìŠµí–ˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "**ê¸°ì–µí•´ì•¼ í•  í•µì‹¬**:\n",
        "- âœ… í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ì˜ ì¤‘ìš”ì„± (í† í°í™”, ì •ê·œí™”, ë¶ˆìš©ì–´ ì œê±°, í‘œì œì–´ ì¶”ì¶œ)\n",
        "- âœ… TF-IDFë¥¼ í™œìš©í•œ íš¨ê³¼ì ì¸ ë²¡í„°í™”\n",
        "- âœ… K-Meansë¥¼ ì´ìš©í•œ ë¬¸ì„œ êµ°ì§‘í™”\n",
        "- âœ… ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ í†µí•œ ë¬¸ì„œ ìœ ì‚¬ì„± ì¸¡ì •\n",
        "\n",
        "**í•™ìŠµ ë°©ë²•**:\n",
        "1. ê° ì…€ì„ ì§ì ‘ ì‹¤í–‰í•´ë³´ì„¸ìš”\n",
        "2. ë‹¤ë¥¸ í…ìŠ¤íŠ¸ ë°ì´í„°ë¡œ ì‹¤ìŠµí•´ë³´ì„¸ìš”\n",
        "3. íŒŒë¼ë¯¸í„°ë¥¼ ë°”ê¿”ê°€ë©° ê²°ê³¼ ë¹„êµí•´ë³´ì„¸ìš”\n",
        "4. ì‹¤ì œ í”„ë¡œì íŠ¸ì— ì ìš©í•´ë³´ì„¸ìš”\n",
        "\n",
        "ì•ìœ¼ë¡œ ë” ê¹Šì´ ìˆëŠ” NLP ê³µë¶€ë¥¼ ìœ„í•´ ìœ„ì˜ í•™ìŠµ ë‹¨ê³„ë¥¼ ì°¸ê³ í•˜ì„¸ìš”! ğŸš€\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ml_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
