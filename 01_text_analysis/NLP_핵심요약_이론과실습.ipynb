{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# NLP 핵심 요약: 이론과 실습\n",
        "\n",
        "이 노트북은 자연어 처리(NLP)의 핵심 개념과 실습을 체계적으로 정리한 자료입니다.\n",
        "앞으로의 공부에 도움이 되도록 이론과 실습 코드를 함께 제공합니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 📚 목차\n",
        "1. [NLP 개요](#nlp-개요)\n",
        "2. [텍스트 전처리](#텍스트-전처리)\n",
        "3. [특성 벡터화](#특성-벡터화)\n",
        "4. [문서 군집화](#문서-군집화)\n",
        "5. [문서 유사도](#문서-유사도)\n",
        "6. [핵심 키워드 정리](#핵심-키워드-정리)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 1. NLP 개요\n",
        "\n",
        "### 🎯 핵심 개념\n",
        "\n",
        "**자연어 처리(NLP)** 란 컴퓨터가 인간의 언어를 이해하고, 생성하며 활용하는 기술입니다.\n",
        "\n",
        "### 📊 NLP 기술 적용 분야\n",
        "\n",
        "#### 1. 자연어 이해\n",
        "- **감성분석** (Sentiment Analysis): 리뷰가 긍정인가 부정인가\n",
        "- **주제분류** (Topic Modeling): 뉴스 내용을 정치/스포츠/과학 등으로 분류\n",
        "- **형태소 분석**: 단어의 형태소 단위로 분해\n",
        "- **개체명인식** (NER): 사람이름/지역 이름 등을 구분\n",
        "- **철자법 교정** (Spelling Correction)\n",
        "- **스팸 탐지** (Spam Detection)\n",
        "\n",
        "#### 2. 자연어 생성\n",
        "- **문장 생성** (Text Generation)\n",
        "- **번역** (Text Translation)\n",
        "- **대화** (Chatbot)\n",
        "- **요약** (Text Summarization)\n",
        "- **이미지/비디오 설명** (Image/Video Captioning)\n",
        "\n",
        "### 🔄 NLP 발전 단계\n",
        "\n",
        "| **구분** | **기존 방식 (2010년대 이전)** | **최근 방식 (딥러닝 기반)** |\n",
        "|----------|------------------------------|---------------------------|\n",
        "| **특징** | 규칙기반, 통계기반, 전통적 ML | Deep Learning, Word Embedding |\n",
        "| **접근법** | 언어학 기반 rule-based | 문맥, 의미, 어순 자동 학습 |\n",
        "| **기술** | Corpus 기반 통계 모델 | Word2Vec, BERT, GPT, Transformer |\n",
        "\n",
        "### 📝 핵심 용어\n",
        "\n",
        "| 용어 | 설명 | 예시 |\n",
        "|------|------|------|\n",
        "| **말뭉치(Corpus)** | 텍스트 데이터의 모음 | 뉴스 기사 모음, 리뷰 데이터 |\n",
        "| **토큰(Token)** | 텍스트의 기본 단위 | \"I love NLP!\" → [\"I\", \"love\", \"NLP\", \"!\"] |\n",
        "| **문장(Sentence)** | 문맥의 단위 | 구두점으로 구분된 텍스트 |\n",
        "| **단어(Word)** | 의미를 가지는 최소 단위 | \"I\", \"love\", \"NLP\" |\n",
        "| **형태소(Morpheme)** | 의미의 최소 단위 | \"unbreakable\" → [\"un-\", \"break\", \"-able\"] |\n",
        "| **불용어(Stopwords)** | 의미 분석에 도움 안되는 단어 | \"the\", \"and\", \"is\", \"은\", \"는\" |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 2. 텍스트 전처리\n",
        "\n",
        "### 🛠️ 전처리 단계\n",
        "\n",
        "1. **텍스트 정규화** (Text Normalization)\n",
        "2. **토큰화** (Tokenization)\n",
        "3. **불용어 제거** (Stopword Removal)\n",
        "4. **어간 추출/표제어 추출** (Stemming/Lemmatization)\n",
        "\n",
        "### 💻 실습 코드\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 필요한 라이브러리 import\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# NLTK 데이터 다운로드\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "print(\"라이브러리 로드 완료!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1 토큰화 (Tokenization)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 예제 텍스트\n",
        "text = 'NLTK is a powerful library for NLP!!!!! I love it.'\n",
        "\n",
        "# 단어 토큰화\n",
        "word_tokens = word_tokenize(text)\n",
        "print(\"단어 토큰화:\", word_tokens)\n",
        "\n",
        "# 문장 토큰화\n",
        "sent_tokens = sent_tokenize(text)\n",
        "print(\"문장 토큰화:\", sent_tokens)\n",
        "\n",
        "# N-gram 생성\n",
        "from nltk import ngrams\n",
        "bigrams = list(ngrams(word_tokens, 2))\n",
        "print(\"Bigram:\", bigrams[:5])  # 처음 5개만 출력\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 감성분석 (Sentiment Analysis)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# VADER 감성분석기 초기화\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "# 감성분석 예제\n",
        "texts = [\n",
        "    'I absolutely love this product!',\n",
        "    'This is okay, I guess',\n",
        "    'I hate it so much',\n",
        "    'The weather is nice today'\n",
        "]\n",
        "\n",
        "for text in texts:\n",
        "    scores = analyzer.polarity_scores(text)\n",
        "    print(f'\\n텍스트: {text}')\n",
        "    print(f'점수: {scores}')\n",
        "    print(f'결과: {\"긍정\" if scores[\"compound\"] > 0.1 else \"부정\" if scores[\"compound\"] < -0.1 else \"중립\"}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.3 불용어 제거 및 정규화\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 텍스트 전처리 함수\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    텍스트 전처리 함수\n",
        "    1. 소문자 변환\n",
        "    2. 문장 부호 제거\n",
        "    3. 토큰화\n",
        "    4. 불용어 제거\n",
        "    5. 어근 추출\n",
        "    \"\"\"\n",
        "    # 1. 소문자 변환\n",
        "    text = text.lower()\n",
        "    \n",
        "    # 2. 문장 부호 제거\n",
        "    punc_rem_dict = dict((ord(ch), None) for ch in string.punctuation)\n",
        "    text = text.translate(punc_rem_dict)\n",
        "    \n",
        "    # 3. 토큰화\n",
        "    tokens = word_tokenize(text)\n",
        "    \n",
        "    # 4. 불용어 제거\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "    \n",
        "    # 5. 어근 추출\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(token, pos='v') for token in tokens]\n",
        "    \n",
        "    return tokens\n",
        "\n",
        "# 전처리 테스트\n",
        "sample_text = 'The Matrix is everywhere its all around us, here even in this room!!!!!'\n",
        "processed = preprocess_text(sample_text)\n",
        "print(\"원본:\", sample_text)\n",
        "print(\"전처리 후:\", processed)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 3. 특성 벡터화\n",
        "\n",
        "### 📊 BOW vs Word Embedding\n",
        "\n",
        "| **구분** | **Bag-of-Words (BOW)** | **Word Embedding** |\n",
        "|----------|------------------------|-------------------|\n",
        "| **개념** | 단어 출현 빈도로 문서 표현 | 단어를 실수 벡터로 표현 |\n",
        "| **특징** | 순서/의미 무시, 고차원 희소 벡터 | 의미적 유사성 반영, 밀집 저차원 벡터 |\n",
        "| **방법** | CountVectorizer, TF-IDF | Word2Vec, GloVe, FastText |\n",
        "| **장점** | 구현 간단, 이해 쉬움 | 문맥 정보 반영, 유사 단어 근접 배치 |\n",
        "| **단점** | 의미 관계/순서 정보 없음 | 많은 데이터/학습 시간 필요 |\n",
        "\n",
        "### 💻 CountVectorizer 실습\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 예제 문서들\n",
        "documents = [\n",
        "    'The Matrix is everywhere its all around us, here even in this room.',\n",
        "    'You take the blue pill and the story ends.',\n",
        "    'You take the red pill and you stay in Wonderland.'\n",
        "]\n",
        "\n",
        "# CountVectorizer 사용\n",
        "count_vectorizer = CountVectorizer(\n",
        "    stop_words='english',  # 불용어 제거\n",
        "    ngram_range=(1, 2),    # 1-gram과 2-gram 사용\n",
        "    max_features=20        # 상위 20개 특성만 사용\n",
        ")\n",
        "\n",
        "# 벡터화\n",
        "count_matrix = count_vectorizer.fit_transform(documents)\n",
        "print(\"문서-특성 행렬 형태:\", count_matrix.shape)\n",
        "print(\"\\n특성 이름들:\")\n",
        "print(count_vectorizer.get_feature_names_out())\n",
        "print(\"\\n벡터화 결과:\")\n",
        "print(count_matrix.toarray())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 📈 TF-IDF 실습\n",
        "\n",
        "**TF-IDF 공식:**\n",
        "- **TF (Term Frequency)**: tf(t,d) = 단어 t의 문서 d 내 등장 횟수 / 문서 d의 전체 단어 수\n",
        "- **IDF (Inverse Document Frequency)**: idf(t) = log(N / (1 + df(t)))\n",
        "- **TF-IDF**: tf-idf(t,d) = tf(t,d) × idf(t)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TfidfVectorizer 사용\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1, 2), max_features=20)\n",
        "\n",
        "# TF-IDF 벡터화\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
        "print(\"TF-IDF 행렬 형태:\", tfidf_matrix.shape)\n",
        "print(\"\\nTF-IDF 점수:\")\n",
        "df_tfidf = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out(), index=[f'문서{i+1}' for i in range(len(documents))])\n",
        "print(df_tfidf.round(3))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 4. 문서 군집화 & 유사도\n",
        "\n",
        "### 🎯 K-Means 클러스터링과 코사인 유사도\n",
        "\n",
        "**핵심 개념:**\n",
        "- **K-Means**: 유사한 문서들을 K개의 그룹으로 분류\n",
        "- **코사인 유사도**: 두 벡터 간 각도로 유사성 측정 (1에 가까울수록 유사)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 샘플 문서 생성\n",
        "sample_docs = [\n",
        "    'The GPS navigation system is very accurate and reliable.',\n",
        "    'Hotel rooms are clean and comfortable with great service.',\n",
        "    'The car battery life is excellent for long trips.',\n",
        "    'Restaurant food quality is outstanding with fresh ingredients.',\n",
        "    'The screen display is clear and bright for reading.',\n",
        "    'Staff members are friendly and helpful at all times.'\n",
        "]\n",
        "\n",
        "# TF-IDF 벡터화\n",
        "tfidf_vec = TfidfVectorizer(stop_words='english', ngram_range=(1, 2), max_df=0.9, min_df=0.1)\n",
        "doc_vecs = tfidf_vec.fit_transform(sample_docs)\n",
        "\n",
        "# K-Means 클러스터링\n",
        "kmeans = KMeans(n_clusters=3, random_state=0)\n",
        "clusters = kmeans.fit_predict(doc_vecs)\n",
        "\n",
        "# 결과 출력\n",
        "doc_df = pd.DataFrame({'문서': sample_docs, '클러스터': clusters})\n",
        "print(\"클러스터링 결과:\")\n",
        "print(doc_df)\n",
        "\n",
        "# 코사인 유사도 계산\n",
        "cosine_sim = cosine_similarity(doc_vecs)\n",
        "print(\"\\n코사인 유사도 행렬:\")\n",
        "sim_df = pd.DataFrame(cosine_sim, index=[f'문서{i+1}' for i in range(len(sample_docs))], columns=[f'문서{i+1}' for i in range(len(sample_docs))])\n",
        "print(sim_df.round(3))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 5. 핵심 키워드 정리\n",
        "\n",
        "### 🔑 반드시 기억해야 할 키워드\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **1. 기본 개념**\n",
        "- **Corpus (말뭉치)**: 분석할 텍스트 데이터 모음\n",
        "- **Token (토큰)**: 텍스트의 기본 분석 단위\n",
        "- **Stopwords (불용어)**: 'the', 'is', 'and' 등 의미 없는 단어\n",
        "- **Lemmatization (표제어 추출)**: 단어의 기본형 추출 (running → run)\n",
        "\n",
        "#### **2. 벡터화 방법**\n",
        "- **CountVectorizer**: 단어 출현 횟수 기반 벡터화\n",
        "- **TF-IDF**: 단어의 중요도 가중치 계산\n",
        "  - **TF** (Term Frequency): 문서 내 단어 빈도\n",
        "  - **IDF** (Inverse Document Frequency): 전체 문서에서의 희소성\n",
        "- **N-gram**: 연속된 N개 단어 조합 (bigram, trigram 등)\n",
        "\n",
        "#### **3. 분석 기법**\n",
        "- **Sentiment Analysis (감성분석)**: 텍스트의 긍정/부정/중립 판단\n",
        "- **K-Means Clustering**: 유사 문서 그룹화\n",
        "- **Cosine Similarity**: 문서 간 유사도 측정 (0~1 사이 값)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 📚 주요 함수 치트시트\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 주요 함수 사용법 정리\n",
        "\n",
        "# 1. 토큰화\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "word_tokenize(\"Hello world!\")  # ['Hello', 'world', '!']\n",
        "sent_tokenize(\"Hi. How are you?\")  # ['Hi.', 'How are you?']\n",
        "\n",
        "# 2. 불용어\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# 3. 표제어 추출\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatizer.lemmatize('running', pos='v')  # 'run'\n",
        "\n",
        "# 4. 감성분석\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "analyzer.polarity_scores(\"I love this!\")  # {'neg': 0.0, 'neu': 0.192, 'pos': 0.808, 'compound': 0.6369}\n",
        "\n",
        "# 5. CountVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cv = CountVectorizer(stop_words='english', ngram_range=(1,2), max_features=100)\n",
        "# cv.fit_transform(documents)\n",
        "\n",
        "# 6. TF-IDF\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf = TfidfVectorizer(stop_words='english', ngram_range=(1,2), max_df=0.85, min_df=0.05)\n",
        "# tfidf.fit_transform(documents)\n",
        "\n",
        "# 7. K-Means\n",
        "from sklearn.cluster import KMeans\n",
        "kmeans = KMeans(n_clusters=3, random_state=0)\n",
        "# kmeans.fit_predict(vectors)\n",
        "\n",
        "# 8. 코사인 유사도\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "# cosine_similarity(vec1, vec2)\n",
        "\n",
        "print(\"주요 함수 사용법을 정리했습니다!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 🎯 실무 적용 팁\n",
        "\n",
        "1. **전처리가 성능의 80%를 결정**\n",
        "   - 소문자 변환, 특수문자 제거, 불용어 제거 필수\n",
        "   - 도메인에 맞는 커스텀 불용어 추가\n",
        "\n",
        "2. **TF-IDF > CountVectorizer**\n",
        "   - 대부분의 경우 TF-IDF가 더 좋은 성능\n",
        "   - 문서의 중요도를 잘 반영\n",
        "\n",
        "3. **N-gram 활용**\n",
        "   - unigram만 사용하면 문맥 손실\n",
        "   - bigram, trigram으로 의미 조합 보존\n",
        "\n",
        "4. **적절한 클러스터 수 선택**\n",
        "   - Elbow method로 최적 K값 찾기\n",
        "   - 도메인 지식 활용\n",
        "\n",
        "5. **전처리 함수 재사용**\n",
        "   - 학습 데이터와 테스트 데이터에 동일한 전처리 적용\n",
        "   - 함수로 만들어 재사용성 높이기\n",
        "\n",
        "text = \"Theater attendance has decreased. The THEATER industry is adapting.\" # 대소문자 통합\n",
        "transformed_text = text.lower()\n",
        "transformed_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 📖 다음 학습 단계\n",
        "\n",
        "#### **중급 단계**\n",
        "1. **Word Embedding**: Word2Vec, GloVe, FastText\n",
        "2. **한국어 NLP**: KoNLPy, 형태소 분석기 (Mecab, Okt, Komoran)\n",
        "3. **실무 도구**: spaCy, Gensim\n",
        "4. **Topic Modeling**: LDA, LSA\n",
        "\n",
        "#### **고급 단계**\n",
        "1. **딥러닝 NLP**: RNN, LSTM, GRU\n",
        "2. **Attention & Transformer**: Self-Attention, Multi-Head Attention\n",
        "3. **최신 모델**: BERT, GPT, T5\n",
        "4. **Fine-tuning**: Pre-trained 모델 활용\n",
        "\n",
        "---\n",
        "\n",
        "## 💡 마무리\n",
        "\n",
        "이 노트북을 통해 NLP의 핵심 개념과 실습을 체계적으로 학습했습니다.\n",
        "\n",
        "**기억해야 할 핵심**:\n",
        "- ✅ 텍스트 전처리의 중요성 (토큰화, 정규화, 불용어 제거, 표제어 추출)\n",
        "- ✅ TF-IDF를 활용한 효과적인 벡터화\n",
        "- ✅ K-Means를 이용한 문서 군집화\n",
        "- ✅ 코사인 유사도를 통한 문서 유사성 측정\n",
        "\n",
        "**학습 방법**:\n",
        "1. 각 셀을 직접 실행해보세요\n",
        "2. 다른 텍스트 데이터로 실습해보세요\n",
        "3. 파라미터를 바꿔가며 결과 비교해보세요\n",
        "4. 실제 프로젝트에 적용해보세요\n",
        "\n",
        "앞으로 더 깊이 있는 NLP 공부를 위해 위의 학습 단계를 참고하세요! 🚀\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ml_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
