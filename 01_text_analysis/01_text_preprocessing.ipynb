{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "612b16bf",
   "metadata": {},
   "source": [
    "# 자연어 처리 NLP (Natural Language Processing) | 텍스트 분석 Text Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee8d357",
   "metadata": {},
   "source": [
    "- 자연어 처리: 사람이 사용하는 언어 전반에 대해서 이해하고 처리하는 분야\n",
    "    - 음성인식, 번역, 감정분석, 질의응답, 언어 생성 등 포괄적 분야\n",
    "- 텍스트 분석: 언어적 비정형 데이터에서 정보를 추출하고 분석하는 작업\n",
    "    - 텍스트 통계적 분석, 주제 분류, 텍스트 군집, 유사도 분석 등"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f9c46a",
   "metadata": {},
   "source": [
    "### 파이썬 텍스트분석 패키지 \n",
    "\n",
    "| **로고 이미지**                                                                                                 | **패키지**   | **설명**                                            | **주요 특징 및 기능**                                                   | **API 문서 URL**                                  |\n",
    "|------------------------------------------------------------------------------------------------------------|--------------|---------------------------------------------------|-----------------------------------------------------------------------|-------------------------------------------------|\n",
    "| ![nltk](https://d.pr/i/9xVzCK+)                   | **nltk**     | 가장 오래된 NLP 라이브러리 중 하나로, 다양한 자연어 처리 도구와 코퍼스 제공 | 토큰화, 품사 태깅, 어간 추출, 불용어 제거, 문법 구조 분석, 감정 분석 등에 유용 | [NLTK API Docs](https://www.nltk.org/api/nltk.html) |\n",
    "| ![gensim](https://radimrehurek.com/gensim/_static/images/gensim.png)                                       | **gensim**   | 주로 텍스트의 토픽 모델링과 문서 유사도 분석을 위한 라이브러리            | Word2Vec, FastText, LDA, 유사도 측정, 대용량 텍스트 처리에 최적화    | [Gensim API Docs](https://radimrehurek.com/gensim/) |\n",
    "| ![spacy](https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/SpaCy_logo.svg/320px-SpaCy_logo.svg.png) | **spacy**    | 빠르고 효율적인 NLP 처리를 위해 개발된 라이브러리로, 산업용 프로젝트에 적합     | 빠른 토큰화, 품사 태깅, NER, 구문 분석, 벡터 표현 제공              | [SpaCy API Docs](https://spacy.io/api)             |\n",
    "| ![TextBlob](https://textblob.readthedocs.io/en/dev/_static/textblob-logo.png)                              | **TextBlob** | 간단한 NLP 작업을 위한 라이브러리로, 감정 분석과 텍스트 정제 등 지원  | 문법 교정, 감정 분석, 텍스트 번역 등과 같은 간단한 작업에 적합      | [TextBlob API Docs](https://textblob.readthedocs.io/en/dev/) |\n",
    "| ![KoNLPy](https://konlpy.org/en/latest/_static/konlpy.png)                                                 | **KoNLPy**   | 한국어 자연어 처리를 위한 라이브러리로, 여러 형태소 분석기를 제공          | Kkma, Hannanum, Komoran, Twitter, Mecab 형태소 분석기 지원            | [KoNLPy API Docs](https://konlpy.org/en/latest/)  |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05219e9f",
   "metadata": {},
   "source": [
    "### NLTK (Natural Language Toolkit)\n",
    "- 파이썬에서 텍스트 처리 및 자연어 처리를 쉽게 다룰 수 있게 해주는 오픈 소스 라이브러리\n",
    "- NLTK는 다양한 언어 리소스와 알고리즘을 포함하고 있으며, 텍스트 마이닝, 텍스트 분석, 그리고 자연어 처리를 공부하거나 구현할 때 유용\n",
    "\n",
    "**주요 기능**\n",
    "1. **토큰화(Tokenization)**: 문장을 단어 또는 문장 단위로 나누는 작업\n",
    "    - 예를 들어, `\"I love NLP.\"`를 `['I', 'love', 'NLP', '.']`와 같이 나누는 기능을 제공한다.\n",
    "2. **품사 태깅(Part-of-Speech Tagging)**: 각 단어에 대해 해당 품사를 태깅하는 작업\n",
    "    - 예를 들어, `\"I love NLP.\"`에 대해 `[('I', 'PRP'), ('love', 'VBP'), ('NLP', 'NNP'), ('.', '.')]`와 같이 태깅한다.\n",
    "3. **명사구 추출(Chunking)**: 문장에서 명사구와 같은 특정 구문을 추출하는 작업\n",
    "4. **어근 추출(Lemmatization) 및 어간 추출(Stemming)**: 단어의 기본 형태를 찾는 작업으로, 동사의 기본형을 찾거나 복수형을 단수형으로 변환하는 등의 작업 수행\n",
    "5. **텍스트 분류(Classification)**: Naive Bayes, MaxEnt 등의 분류 모델을 사용해 텍스트 분류 가능\n",
    "6. **코퍼스(corpus) 제공**: 영화 리뷰, 뉴스 기사 등 여러 텍스트 데이터셋을 포함하고 있어 학습과 실습에 유용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c824c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 channel Terms of Service accepted\n",
      "Retrieving notices: done\n",
      "Channels:\n",
      " - defaults\n",
      "Platform: win-64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: C:\\Users\\Playdata\\anaconda3\\envs\\ml_env\n",
      "\n",
      "  added / updated specs:\n",
      "    - nltk\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    ca-certificates-2025.9.9   |       haa95532_0         127 KB\n",
      "    click-8.2.1                |  py312haa95532_0         329 KB\n",
      "    colorama-0.4.6             |  py312haa95532_0          53 KB\n",
      "    joblib-1.5.2               |  py312haa95532_0         517 KB\n",
      "    nltk-3.9.1                 |  py312haa95532_0         2.7 MB\n",
      "    openssl-3.0.18             |       h543e019_0         6.8 MB\n",
      "    regex-2025.9.1             |  py312h02ab6af_0         364 KB\n",
      "    tqdm-4.67.1                |  py312hfc267ef_0         187 KB\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:        11.1 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  click              pkgs/main/win-64::click-8.2.1-py312haa95532_0 \n",
      "  colorama           pkgs/main/win-64::colorama-0.4.6-py312haa95532_0 \n",
      "  joblib             pkgs/main/win-64::joblib-1.5.2-py312haa95532_0 \n",
      "  nltk               pkgs/main/win-64::nltk-3.9.1-py312haa95532_0 \n",
      "  regex              pkgs/main/win-64::regex-2025.9.1-py312h02ab6af_0 \n",
      "  tqdm               pkgs/main/win-64::tqdm-4.67.1-py312hfc267ef_0 \n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  ca-certificates                      2025.7.15-haa95532_0 --> 2025.9.9-haa95532_0 \n",
      "  openssl                                 3.0.17-h35632f6_0 --> 3.0.18-h543e019_0 \n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages: ...working...\n",
      "openssl-3.0.18       | 6.8 MB    |            |   0% \n",
      "\n",
      "nltk-3.9.1           | 2.7 MB    |            |   0% \u001b[A\n",
      "\n",
      "\n",
      "joblib-1.5.2         | 517 KB    |            |   0% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "regex-2025.9.1       | 364 KB    |            |   0% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "click-8.2.1          | 329 KB    |            |   0% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "tqdm-4.67.1          | 187 KB    |            |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ca-certificates-2025 | 127 KB    |            |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "colorama-0.4.6       | 53 KB     |            |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "nltk-3.9.1           | 2.7 MB    |            |   1% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "click-8.2.1          | 329 KB    | 4          |   5% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "joblib-1.5.2         | 517 KB    | 3          |   3% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "regex-2025.9.1       | 364 KB    | 4          |   4% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "click-8.2.1          | 329 KB    | ########## | 100% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "joblib-1.5.2         | 517 KB    | ########## | 100% \u001b[A\u001b[A\n",
      "openssl-3.0.18       | 6.8 MB    |            |   0% \n",
      "\n",
      "\n",
      "\n",
      "regex-2025.9.1       | 364 KB    | ########## | 100% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "nltk-3.9.1           | 2.7 MB    | ########## | 100% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "tqdm-4.67.1          | 187 KB    | 8          |   9% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ca-certificates-2025 | 127 KB    | #2         |  13% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "tqdm-4.67.1          | 187 KB    | ########## | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ca-certificates-2025 | 127 KB    | ########## | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "colorama-0.4.6       | 53 KB     | ###        |  30% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "colorama-0.4.6       | 53 KB     | ########## | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "regex-2025.9.1       | 364 KB    | ########## | 100% \u001b[A\u001b[A\u001b[A\n",
      "openssl-3.0.18       | 6.8 MB    | ###4       |  34% \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "click-8.2.1          | 329 KB    | ########## | 100% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "click-8.2.1          | 329 KB    | ########## | 100% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "openssl-3.0.18       | 6.8 MB    | #########2 |  92% \n",
      "openssl-3.0.18       | 6.8 MB    | ########## | 100% \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "tqdm-4.67.1          | 187 KB    | ########## | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "tqdm-4.67.1          | 187 KB    | ########## | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "joblib-1.5.2         | 517 KB    | ########## | 100% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "joblib-1.5.2         | 517 KB    | ########## | 100% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ca-certificates-2025 | 127 KB    | ########## | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ca-certificates-2025 | 127 KB    | ########## | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "colorama-0.4.6       | 53 KB     | ########## | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "colorama-0.4.6       | 53 KB     | ########## | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "openssl-3.0.18       | 6.8 MB    | ########## | 100% \n",
      "\n",
      "nltk-3.9.1           | 2.7 MB    | ########## | 100% \u001b[A\n",
      "\n",
      "nltk-3.9.1           | 2.7 MB    | ########## | 100% \u001b[A\n",
      "                                                     \n",
      "\n",
      "\n",
      "                                                     \u001b[A\n",
      "\n",
      "\n",
      "                                                     \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "                                                     \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                     \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                     \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                     \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                     \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A done\n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "    current version: 25.5.1\n",
      "    latest version: 25.9.1\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# !conda install nltk -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b59e0ea0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.9.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ceac9c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Playdata\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Playdata\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Playdata\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk 리소스 다운로드\n",
    "nltk.download('punkt')       # 토큰화에 필요한 데이터\n",
    "nltk.download('punkt_tab')   #\n",
    "nltk.download('stopwords')   # 불용어 리스트 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46d11f32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NLTK',\n",
       " 'is',\n",
       " 'a',\n",
       " 'powerful',\n",
       " 'library',\n",
       " 'for',\n",
       " 'NLP',\n",
       " '!',\n",
       " '!',\n",
       " '!',\n",
       " '!',\n",
       " '!']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "text = 'NLTK is a powerful library for NLP!!!!!'\n",
    "word_tokenize(text)    # 단어 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a872015",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Playdata\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 감성분석\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5495476",
   "metadata": {},
   "source": [
    "### SentimentIntensityAnalyzer() 감정분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb47a88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 0.259, 'pos': 0.741, 'compound': 0.8619}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SentimentIntensityAnalyzer를 통한 감성분석\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "analyser = SentimentIntensityAnalyzer()\n",
    "\n",
    "# vader 는 감정에 대한 사전이 있다. 그래서 그 감정에 대해서 점수를 환산한다.\n",
    "# SentimentIntensityAnalyzer.polarity_scores(corpus)\n",
    "# - neg 부정(0 ~ 1)\n",
    "# - neu 중립(0 ~ 1)\n",
    "# - pos 긍정(0 ~ 1)\n",
    "# - compound 복합(-1 ~ 1)\n",
    "\n",
    "analyser.polarity_scores(\"I love this product! It's amazing!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "81251161",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.6369}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyser.polarity_scores(\"Love\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b206a4dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 1.0, 'neu': 0.0, 'pos': 0.0, 'compound': -0.5106}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyser.polarity_scores(\"Ugly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "553ec924",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.4215}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyser.polarity_scores(\"Satisfied\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "87e0300c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 1.0, 'neu': 0.0, 'pos': 0.0, 'compound': -0.5994}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyser.polarity_scores(\"Death\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "87355115",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyser.polarity_scores(\"soso\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b515d836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I absolutely love this! | {'neg': 0.0, 'neu': 0.295, 'pos': 0.705, 'compound': 0.6989}\n",
      "This is okay, I guess | {'neg': 0.0, 'neu': 0.612, 'pos': 0.388, 'compound': 0.2263}\n",
      "I hate it so much | {'neg': 0.552, 'neu': 0.448, 'pos': 0.0, 'compound': -0.5719}\n"
     ]
    }
   ],
   "source": [
    "texts = ['I absolutely love this!',\n",
    "         'This is okay, I guess',\n",
    "         'I hate it so much'\n",
    "         ]\n",
    "\n",
    "for text in texts:\n",
    "    score = analyser.polarity_scores(text)\n",
    "    print(f'{text} | {score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3da1829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'don', \"'\", 't', 'like', 'the', 'way', 'she', 'talks', 'about', 'mental', 'well', '-', 'being', '.']\n",
      "========================================================================================================================\n",
      "I don ' t like the way she talks about mental well - being .\n",
      "{'neg': 0.0, 'neu': 0.635, 'pos': 0.365, 'compound': 0.5574}\n"
     ]
    }
   ],
   "source": [
    "# 토큰화 결과가 감성분석에 끼치는 영향\n",
    "\n",
    "# word_tokenize, sent_tokenize\n",
    "# WordPunctTokenizer : 조금 더 세밀하게 토큰처리 -> 특수문자도 토큰 처리\n",
    "\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "corpus = \"I don't like the way she talks about mental well-being.\"\n",
    "\n",
    "\n",
    "analyser = SentimentIntensityAnalyzer()\n",
    "\n",
    "# WordPunctTokenizer: 특수문자도 토큰처리\n",
    "word_punct_tokenizer = WordPunctTokenizer()\n",
    "tokens1 = word_punct_tokenizer.tokenize(corpus)\n",
    "print(tokens1)\n",
    "\n",
    "print(\"===\" * 40)\n",
    "\n",
    "# 문장으로 결함, SentimentIntensityAnalyzer는 문장에 대해서 '감정'을 수치화하기\n",
    "text1 = ' '.join(tokens1)\n",
    "print(text1)\n",
    "print(analyser.polarity_scores(text1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534ff268",
   "metadata": {},
   "source": [
    "### word_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00decac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'do', \"n't\", 'like', 'the', 'way', 'she', 'talks', 'about', 'mental', 'well-being', '.']\n",
      "========================================================================================================================\n",
      "I do n't like the way she talks about mental well-being .\n",
      "{'neg': 0.19, 'neu': 0.81, 'pos': 0.0, 'compound': -0.2755}\n"
     ]
    }
   ],
   "source": [
    "tokens2 = word_tokenize(corpus)\n",
    "print(tokens2)\n",
    "\n",
    "print(\"===\" * 40)\n",
    "\n",
    "text2 = ' '.join(tokens2)\n",
    "print(text2)\n",
    "print(analyser.polarity_scores(text2))\n",
    "# {'neg': 0.19, 'neu': 0.81, 'pos': 0.0, 'compound': -0.2755}\n",
    "#  n't -> 부정도 해석석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe85fffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -> word_tokenize() 는 '감정', '의미' 로 분리된 것으로 보인다. 사람이 이해하는 방향으로 토큰화\n",
    "# \"n't\", 'well-being' "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335ae0d2",
   "metadata": {},
   "source": [
    "### sent_tokenize(): 문장을 토큰화하기 -> 구두점 '.' 을 기준으로 잘랐구나?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d91837ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The Matrix is everywhere its all around us, here even in this room.',\n",
       " 'You can see it out your window or on your television.',\n",
       " 'You feel it when you go to work, or go to church or pay your taxes.']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '''The Matrix is everywhere its all around us, here even in this room.\n",
    "You can see it out your window or on your television.\n",
    "You feel it when you go to work, or go to church or pay your taxes.'''\n",
    "\n",
    "sent_tokenize(text)     # 문장 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cd60ed53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['The',\n",
       "  'Matrix',\n",
       "  'is',\n",
       "  'everywhere',\n",
       "  'its',\n",
       "  'all',\n",
       "  'around',\n",
       "  'us',\n",
       "  ',',\n",
       "  'here',\n",
       "  'even',\n",
       "  'in',\n",
       "  'this',\n",
       "  'room',\n",
       "  '.'],\n",
       " ['You',\n",
       "  'can',\n",
       "  'see',\n",
       "  'it',\n",
       "  'out',\n",
       "  'your',\n",
       "  'window',\n",
       "  'or',\n",
       "  'on',\n",
       "  'your',\n",
       "  'television',\n",
       "  '.'],\n",
       " ['You',\n",
       "  'feel',\n",
       "  'it',\n",
       "  'when',\n",
       "  'you',\n",
       "  'go',\n",
       "  'to',\n",
       "  'work',\n",
       "  ',',\n",
       "  'or',\n",
       "  'go',\n",
       "  'to',\n",
       "  'church',\n",
       "  'or',\n",
       "  'pay',\n",
       "  'your',\n",
       "  'taxes',\n",
       "  '.']]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문장별 단어 토큰화\n",
    "def tokenize_text(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    return [word_tokenize(sentence) for sentence in sentences]\n",
    "\n",
    "tokenize_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598bf15f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object ngrams at 0x0000018F9FFAD540>\n",
      "[('The', 'Matrix'), ('Matrix', 'is'), ('is', 'everywhere'), ('everywhere', 'its'), ('its', 'all'), ('all', 'around'), ('around', 'us'), ('us', ','), (',', 'here'), ('here', 'even'), ('even', 'in'), ('in', 'this'), ('this', 'room'), ('room', '.')]\n",
      "<generator object ngrams at 0x0000018F9FFAD840>\n",
      "[('The', 'Matrix', 'is'), ('Matrix', 'is', 'everywhere'), ('is', 'everywhere', 'its'), ('everywhere', 'its', 'all'), ('its', 'all', 'around'), ('all', 'around', 'us'), ('around', 'us', ','), ('us', ',', 'here'), (',', 'here', 'even'), ('here', 'even', 'in'), ('even', 'in', 'this'), ('in', 'this', 'room'), ('this', 'room', '.')]\n"
     ]
    }
   ],
   "source": [
    "# n-gram\n",
    "# window slicing 방식으로 단어들이 '문맥의 정보'를 반영할 수 있겠금,창을 옆으로 이동하면서 n개의 묶어서 하나의 토큰으로 묶는다. 하나의 토큰은 하나의 '튜플'형태를 이룬다.\n",
    "\n",
    "from nltk import ngrams\n",
    "\n",
    "text = 'The Matrix is everywhere its all around us, here even in this room.'\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "bigram = ngrams(tokens, 2)\n",
    "print(bigram)\n",
    "print([token for token in bigram])\n",
    "\n",
    "trigram = ngrams(tokens, 3)\n",
    "print(trigram)\n",
    "print([token for token in trigram])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a3ccb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<WordListCorpusReader in 'C:\\\\Users\\\\Playdata\\\\AppData\\\\Roaming\\\\nltk_data\\\\corpora\\\\stopwords'>\n",
      "['albanian', 'arabic', 'azerbaijani', 'basque', 'belarusian', 'bengali', 'catalan', 'chinese', 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'greek', 'hebrew', 'hinglish', 'hungarian', 'indonesian', 'italian', 'kazakh', 'nepali', 'norwegian', 'portuguese', 'romanian', 'russian', 'slovene', 'spanish', 'swedish', 'tajik', 'tamil', 'turkish']\n",
      "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", 'her', 'here', 'hers', 'herself', \"he's\", 'him', 'himself', 'his', 'how', 'i', \"i'd\", 'if', \"i'll\", \"i'm\", 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', \"i've\", 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', 'shouldn', \"shouldn't\", \"should've\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", 'were', 'weren', \"weren't\", \"we've\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", 'your', \"you're\", 'yours', 'yourself', 'yourselves', \"you've\"]\n",
      "198\n"
     ]
    }
   ],
   "source": [
    "# 불용어(stopwords) 제거\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "print(stopwords)                   # 객체\n",
    "print(stopwords.fileids())         # 지원되는 언어 목록 : 어떤 언어로 되어있는지\n",
    "print(stopwords.words('english'))  # 영어 불용어 목록 : 영어에는 어떤 '불용어' stopwords 가 있는지\n",
    "print(len(stopwords.words('english')))  # 영어 '불용어' 개수\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a2f379",
   "metadata": {},
   "source": [
    "1. **텍스트 정규화(Text Normalization)**\n",
    "    - **목적**: 텍스트를 일관된 형태로 변환하여 처리 용이성을 높인다.\n",
    "    - **주요 작업**\n",
    "        - 모든 문자 소문자 변환 또는 대문자 변환\n",
    "        - 숫자, 특수 문자 제거 또는 대체\n",
    "        - 불필요한 공백 제거\n",
    "2. **토큰화(Tokenization)**\n",
    "    - **목적**: 문장을 단어 또는 하위 단위(subword)로 분할하여 분석한다.\n",
    "    - **방법**:\n",
    "        - 단순 공백 기준 분할\n",
    "        - 형태소 분석기를 사용한 분할\n",
    "3. **불용어 제거(Stopword Removal)**\n",
    "    - **목적**: 의미에 크게 기여하지 않는 단어를 제거하여 데이터의 노이즈를 줄인다.\n",
    "    - **예시 불용어**: \"은\", \"는\", \"이\", \"가\", \"and\", \"the\" 등\n",
    "4. **어간 추출 및 표제어 추출(Stemming and Lemmatization)**\n",
    "    - **목적**: 단어의 기본 형태를 추출하여 단어 수를 줄이고 일반화한다.\n",
    "    - **예시:** \"하는\", \"했다\", \"한다\" → \"하다\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7e8d34a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['matrix', 'everywhere', 'around', 'us', ',', 'even', 'room', '.']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'The Matrix is everywhere its all around us, here even in this room.'\n",
    "\n",
    "stopwords_list = stopwords.words('english')\n",
    "\n",
    "# 토큰화\n",
    "tokens = word_tokenize(text)\n",
    "# 소문자 변환\n",
    "tokens = [token.lower() for token in tokens]\n",
    "# 불용어 제거\n",
    "tokens = [token for token in tokens if token not in stopwords_list]\n",
    "\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "20b5e7ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['matrix', 'everywhere', 'around', 'us', ',', 'even', 'room', '.']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'The Matrix is everywhere its all around us, here even in this room.'\n",
    "\n",
    "stopwords_list = stopwords.words('english')\n",
    "\n",
    "tokens = []\n",
    "\n",
    "for word in word_tokenize(text):       # 토큰화\n",
    "    word = word.lower()               # 소문자 변환\n",
    "    if word not in stopwords_list:    # 불용어 처리\n",
    "        tokens.append(word)    \n",
    "\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2279dbc1",
   "metadata": {},
   "source": [
    "### 특성 벡터화 Feature Vectorization\n",
    "1. BOW(Bag of Words): 문서가 가지는 모든 단어를 문맥이나 순서를 무시하고 일괄적으로 단어에 대해 **빈도** 값을 부여해 피처 값을 추출하는 모델이다.\n",
    "    \n",
    "   <img src=\"https://miro.medium.com/v2/resize:fit:1400/1*S8uW62e3AbYw3gnylm4eDg.png\" width=\"500px\">\n",
    "\n",
    "2. Word Embedding: 단어를 밀집 벡터(dense vector)로 표현하는 방법으로, 단어의 의미와 관계를 보존하며 벡터로 표현한다.\n",
    "    \n",
    "    <img src=\"https://miro.medium.com/v2/resize:fit:1400/1*jpnKO5X0Ii8PVdQYFO2z1Q.png\" width=\"500px\">\n",
    "\n",
    "\n",
    "| **구분**             | **Bag-of-Words (BOW)**                             | **Word Embedding**                             |\n",
    "|----------------------|--------------------------------------------------|------------------------------------------------|\n",
    "| **개념**             | 문서를 단어의 출현 빈도로 표현                   | 단어를 실수 벡터로 표현                       |\n",
    "| **특징**             | - 단어의 순서와 의미를 고려하지 않음             | - 단어 간 의미적 유사성을 반영                |\n",
    "|                      | - 고차원, 희소 벡터 생성                         | - 밀집된 저차원 벡터 생성                     |\n",
    "| **대표 방법**        | Count Vector, TF-IDF                             | Word2Vec, GloVe, FastText                     |\n",
    "| **장점**             | - 구현이 간단하고 이해하기 쉬움                  | - 문맥 정보 반영 가능                         |\n",
    "|                      | - 단순 텍스트 데이터 분석에 유용                 | - 유사한 단어를 벡터 공간 상에서 가깝게 위치 |\n",
    "| **단점**             | - 의미적 관계와 단어의 순서 정보 없음            | - 많은 데이터와 학습 시간 필요                |\n",
    "|                      | - 고차원 희소 벡터 문제                          | - 구현이 상대적으로 복잡                      |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcb20db",
   "metadata": {},
   "source": [
    "- BOW > CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "79cf4c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = 'The Matrix is everywhere its all around us, here even in this room. \\\n",
    "You can see it out your window or on your television. \\\n",
    "You feel it when you go to work, or go to church or pay your taxes.'\n",
    "\n",
    "text2 = 'You take the blue pill and the story ends.  You wake in your bed and you believe whatever you want to believe \\\n",
    "You take the red pill and you stay in Wonderland and I show you how deep the rabbit-hole goes.'\n",
    "\n",
    "texts = [text1, text2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d81abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse._csr.csr_matrix'> <class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 2, 0, 1, 0, 0, 1, 1, 2, 1,\n",
       "        1, 1, 3, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 2, 1, 0, 0,\n",
       "        0, 1, 1, 0, 1, 3, 3],\n",
       "       [0, 4, 0, 1, 2, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 2, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 1, 1, 1, 2, 0, 0, 4, 0, 1, 0, 1, 1,\n",
       "        1, 0, 0, 1, 0, 7, 1]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# CountVectorizer는 문서(문장 집합)에서 사용된 모든 단어의 빈도를 세어,\n",
    "# 각 문서를 '단어 등장 빈도' 기반의 벡터로 변환하는 도구입니다.\n",
    "count_vectorizer = CountVectorizer()\n",
    "\n",
    "# fit()을 실행하면, 전체 텍스트에서 고유한 단어(토큰) 목록을 뽑아\n",
    "# '단어 사전(vocabulary)'을 내부적으로 생성합니다.\n",
    "count_vectorizer.fit(texts)\n",
    "\n",
    "# transform()은 texts(문장 집합)를 \n",
    "# 각 문서별로 단어가 몇 번 나왔는지 세어 벡터로 변환합니다.\n",
    "text_vecs = count_vectorizer.transform(texts)\n",
    "\n",
    "# 변환 결과 객체의 데이터 타입을 출력해봅니다.\n",
    "# 희소 행렬(효율적인 저장 방식)과 numpy 배열 타입임을 알 수 있습니다.\n",
    "print(type(text_vecs), type(text_vecs.toarray()))\n",
    "# <class 'scipy.sparse._csr.csr_matrix'> <class 'numpy.ndarray'>\n",
    "\n",
    "# 실제 CountVectorizer가 만든 희소 행렬 자체를 출력합니다.\n",
    "text_vecs\n",
    "# 실제 출력 예시:\n",
    "# <2x51 sparse matrix of type '<class 'numpy.int64'>'\n",
    "# \twith 58 stored elements in Compressed Sparse Row format>\n",
    "# (2개의 문서, 51개의 고유 단어(특성)로 구성된 희소 행렬)\n",
    "\n",
    "# 희소 행렬을 일반적인 2차원 numpy 배열로 변환해서 출력해 봅니다.\n",
    "# 각 행이 문서, 각 열이 단어에 해당하며,\n",
    "# 위치의 값은 그 단어가 문서에서 등장한 횟수입니다.\n",
    "text_vecs.toarray()\n",
    "# 실제 출력 예시:\n",
    "# array([[1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 2, 0, 1, 0, 0, 1, 1, 2, 1,\n",
    "#         1, 1, 3, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 2, 1, 0, 0,\n",
    "#         0, 1, 1, 0, 1, 3, 3],\n",
    "#        [0, 4, 0, 1, 2, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 2, 0, 0, 0,\n",
    "#         0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 1, 1, 1, 2, 0, 0, 4, 0, 1, 0, 1, 1,\n",
    "#         1, 0, 0, 1, 0, 7, 1]])\n",
    "# (첫 번째 행: 첫 번째 문서의 단어별 등장 횟수, 두 번째 행: 두 번째 문서의 단어별 등장 횟수)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "babf3b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['all' 'and' 'around' 'bed' 'believe' 'blue' 'can' 'church' 'deep' 'ends'\n",
      " 'even' 'everywhere' 'feel' 'go' 'goes' 'here' 'hole' 'how' 'in' 'is' 'it'\n",
      " 'its' 'matrix' 'on' 'or' 'out' 'pay' 'pill' 'rabbit' 'red' 'room' 'see'\n",
      " 'show' 'stay' 'story' 'take' 'taxes' 'television' 'the' 'this' 'to' 'us'\n",
      " 'wake' 'want' 'whatever' 'when' 'window' 'wonderland' 'work' 'you' 'your']\n",
      "{'the': 38, 'matrix': 22, 'is': 19, 'everywhere': 11, 'its': 21, 'all': 0, 'around': 2, 'us': 41, 'here': 15, 'even': 10, 'in': 18, 'this': 39, 'room': 30, 'you': 49, 'can': 6, 'see': 31, 'it': 20, 'out': 25, 'your': 50, 'window': 46, 'or': 24, 'on': 23, 'television': 37, 'feel': 12, 'when': 45, 'go': 13, 'to': 40, 'work': 48, 'church': 7, 'pay': 26, 'taxes': 36, 'take': 35, 'blue': 5, 'pill': 27, 'and': 1, 'story': 34, 'ends': 9, 'wake': 42, 'bed': 3, 'believe': 4, 'whatever': 44, 'want': 43, 'red': 29, 'stay': 33, 'wonderland': 47, 'show': 32, 'how': 17, 'deep': 8, 'rabbit': 28, 'hole': 16, 'goes': 14}\n",
      "51\n"
     ]
    }
   ],
   "source": [
    "print(count_vectorizer.get_feature_names_out())\n",
    "print(count_vectorizer.vocabulary_)    # key,value를 가지고 있는 dict\n",
    "# key : 단어 사전에서 만들어진 단어\n",
    "# value : 만들어진 단어의 index 번호\n",
    "print(len(count_vectorizer.get_feature_names_out()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "05470bda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>all</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>and</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>around</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bed</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>believe</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>blue</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>can</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>church</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>deep</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ends</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>even</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>everywhere</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>feel</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>go</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>goes</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>here</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>hole</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>how</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>in</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>is</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>it</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>its</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>matrix</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>on</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>or</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>out</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>pay</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>pill</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>rabbit</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>red</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>room</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>see</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>show</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>stay</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>story</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>take</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>taxes</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>television</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>the</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>this</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>to</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>us</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>wake</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>want</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>whatever</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>when</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>window</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>wonderland</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>work</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>you</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>your</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          word  idx\n",
       "0          all    0\n",
       "1          and    1\n",
       "2       around    2\n",
       "3          bed    3\n",
       "4      believe    4\n",
       "5         blue    5\n",
       "6          can    6\n",
       "7       church    7\n",
       "8         deep    8\n",
       "9         ends    9\n",
       "10        even   10\n",
       "11  everywhere   11\n",
       "12        feel   12\n",
       "13          go   13\n",
       "14        goes   14\n",
       "15        here   15\n",
       "16        hole   16\n",
       "17         how   17\n",
       "18          in   18\n",
       "19          is   19\n",
       "20          it   20\n",
       "21         its   21\n",
       "22      matrix   22\n",
       "23          on   23\n",
       "24          or   24\n",
       "25         out   25\n",
       "26         pay   26\n",
       "27        pill   27\n",
       "28      rabbit   28\n",
       "29         red   29\n",
       "30        room   30\n",
       "31         see   31\n",
       "32        show   32\n",
       "33        stay   33\n",
       "34       story   34\n",
       "35        take   35\n",
       "36       taxes   36\n",
       "37  television   37\n",
       "38         the   38\n",
       "39        this   39\n",
       "40          to   40\n",
       "41          us   41\n",
       "42        wake   42\n",
       "43        want   43\n",
       "44    whatever   44\n",
       "45        when   45\n",
       "46      window   46\n",
       "47  wonderland   47\n",
       "48        work   48\n",
       "49         you   49\n",
       "50        your   50"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "vocab = sorted(count_vectorizer.vocabulary_.items(), key=lambda x:x[1])\n",
    "vocab_df = pd.DataFrame(vocab, columns=['word', 'idx'])\n",
    "display(vocab_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b3d798c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>all</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>and</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>around</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bed</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>believe</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>blue</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>can</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>church</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>deep</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ends</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>even</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>everywhere</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>feel</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>go</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>goes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>here</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>hole</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>how</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>in</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>is</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>it</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>its</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>matrix</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>on</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>or</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>out</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>pay</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>pill</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>rabbit</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>red</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>room</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>see</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>show</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>stay</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>story</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>take</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>taxes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>television</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>the</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>this</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>to</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>us</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>wake</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>want</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>whatever</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>when</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>window</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>wonderland</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>work</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>you</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>your</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          word  count\n",
       "0          all      1\n",
       "1          and      4\n",
       "2       around      1\n",
       "3          bed      1\n",
       "4      believe      2\n",
       "5         blue      1\n",
       "6          can      1\n",
       "7       church      1\n",
       "8         deep      1\n",
       "9         ends      1\n",
       "10        even      1\n",
       "11  everywhere      1\n",
       "12        feel      1\n",
       "13          go      2\n",
       "14        goes      1\n",
       "15        here      1\n",
       "16        hole      1\n",
       "17         how      1\n",
       "18          in      3\n",
       "19          is      1\n",
       "20          it      2\n",
       "21         its      1\n",
       "22      matrix      1\n",
       "23          on      1\n",
       "24          or      3\n",
       "25         out      1\n",
       "26         pay      1\n",
       "27        pill      2\n",
       "28      rabbit      1\n",
       "29         red      1\n",
       "30        room      1\n",
       "31         see      1\n",
       "32        show      1\n",
       "33        stay      1\n",
       "34       story      1\n",
       "35        take      2\n",
       "36       taxes      1\n",
       "37  television      1\n",
       "38         the      5\n",
       "39        this      1\n",
       "40          to      3\n",
       "41          us      1\n",
       "42        wake      1\n",
       "43        want      1\n",
       "44    whatever      1\n",
       "45        when      1\n",
       "46      window      1\n",
       "47  wonderland      1\n",
       "48        work      1\n",
       "49         you     10\n",
       "50        your      4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 단어별 등장 횟수 구하기하기\n",
    "word_counts = text_vecs.toarray().sum(axis=0)\n",
    "# text_vecs를 배열형태로 변형하여 합해보자\n",
    "\n",
    "vocab_df['count'] = vocab_df['idx'].apply(lambda i:word_counts[i])\n",
    "\n",
    "vocab_df = vocab_df.drop(columns=['idx'])\n",
    "display(vocab_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ed2c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 24)\n"
     ]
    }
   ],
   "source": [
    "# CountVectorizer(stop_words='언어'): 불용어 제거\n",
    "\n",
    "count_vectorizer = CountVectorizer(stop_words='english')\n",
    "texts_vecs = count_vectorizer.fit_transform(texts)\n",
    "print(texts_vecs.toarray().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9b2142a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bed</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>believe</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blue</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>church</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>deep</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ends</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>feel</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>goes</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>hole</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>matrix</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>pay</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>pill</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>rabbit</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>red</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>room</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>stay</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>story</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>taxes</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>television</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>wake</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>want</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>window</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>wonderland</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>work</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          word  idx\n",
       "0          bed    0\n",
       "1      believe    1\n",
       "2         blue    2\n",
       "3       church    3\n",
       "4         deep    4\n",
       "5         ends    5\n",
       "6         feel    6\n",
       "7         goes    7\n",
       "8         hole    8\n",
       "9       matrix    9\n",
       "10         pay   10\n",
       "11        pill   11\n",
       "12      rabbit   12\n",
       "13         red   13\n",
       "14        room   14\n",
       "15        stay   15\n",
       "16       story   16\n",
       "17       taxes   17\n",
       "18  television   18\n",
       "19        wake   19\n",
       "20        want   20\n",
       "21      window   21\n",
       "22  wonderland   22\n",
       "23        work   23"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vocab = sorted(count_vectorizer.vocabulary_.items(), key=lambda x:x[1])\n",
    "vocab_df = pd.DataFrame(vocab, columns=['word', 'idx'])\n",
    "display(vocab_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d450b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 48)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['bed', 'bed believe', 'believe', 'believe red', 'believe want',\n",
       "       'blue', 'blue pill', 'church', 'church pay', 'deep', 'deep rabbit',\n",
       "       'ends', 'ends wake', 'feel', 'feel work', 'goes', 'hole',\n",
       "       'hole goes', 'matrix', 'matrix room', 'pay', 'pay taxes', 'pill',\n",
       "       'pill stay', 'pill story', 'rabbit', 'rabbit hole', 'red',\n",
       "       'red pill', 'room', 'room window', 'stay', 'stay wonderland',\n",
       "       'story', 'story ends', 'taxes', 'television', 'television feel',\n",
       "       'wake', 'wake bed', 'want', 'want believe', 'window',\n",
       "       'window television', 'wonderland', 'wonderland deep', 'work',\n",
       "       'work church'], dtype=object)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ngram_range 까지 추가해보기\n",
    "count_vectorizer = CountVectorizer(\n",
    "    stop_words='english',\n",
    "    ngram_range=(1, 2), # n-gram 범위 지정 (최소값, 최대값)\n",
    "    max_features=20     # 빈도수 상위인 n개의 데이터를 사용\n",
    "    )\n",
    "texts_vecs = count_vectorizer.fit_transform(texts)\n",
    "print(texts_vecs.toarray().shape)\n",
    "\n",
    "count_vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e55575",
   "metadata": {},
   "source": [
    "- BOW > TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8038546d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 강사님 설명 메모중...(아래는 기존 메모입니다. - 필요하면 참고)\n",
    "# 내 문장 안에서의 빈도는 Tf 높아야 하지만,\n",
    "# 전체 문장에서 빈도가 높다는 것은 큰 의미가 없다?\n",
    "# 다른 문장과 함께 일때는 이 단어에 대한 빈도가 낮아야 Tf 에서의 의미가 더 중요하게 인식된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3608f180",
   "metadata": {},
   "source": [
    "## CountVectorizer와 TfidfVectorizer의 차이점\n",
    "\n",
    "- **CountVectorizer**\n",
    "    - 각 문서에서 단어가 등장한 횟수(빈도)를 단순히 세는 방법입니다.\n",
    "    - 예시: 'apple apple banana'라는 문장이 있을 때, `{'apple': 2, 'banana': 1}`와 같이 카운트합니다.\n",
    "    - 빈도 정보만 반영할 뿐, 단어가 다른 문서들에서 얼마나 드물게 등장했는지는 고려하지 않습니다.\n",
    "\n",
    "- **TfidfVectorizer**\n",
    "    - \"등장 빈도(TF, Term Frequency)\"와 \"역문서 빈도(IDF, Inverse Document Frequency)\"를 곱해서 각 단어의 가중치를 계산합니다.\n",
    "    - **TF**: 특정 단어가 하나의 문서 내에서 얼마나 자주 등장하는지를 의미합니다.\n",
    "    - **IDF**: 해당 단어가 전체 문서 집합에서 얼마나 드물게 등장하는지를 의미합니다.\n",
    "    - 한 문서에서 많이 등장하지만 다른 문서들에선 자주 나오지 않는 단어일수록 TF-IDF 점수가 높아집니다.\n",
    "    - 반대로 자주 등장하는 불용어(예: 'and', 'the')는 TF는 높을 수 있어도 IDF가 매우 낮아서 전체 TF-IDF 점수도 낮게 나옵니다.\n",
    "\n",
    "---\n",
    "\n",
    "### BOW > TfidfVectorizer의 의미\n",
    "\n",
    "- **BOW(CountVectorizer) 방식보다, TfidfVectorizer 방식이 더 진화된 방식임을 의미합니다.**\n",
    "- TfidfVectorizer는 단어의 빈도뿐 아니라, 각 단어의 \"중요도\"까지 세밀하게 반영한다는 점에서 BoW보다 더 나은 벡터화 방법입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9274793",
   "metadata": {},
   "source": [
    "- TF-IDF == Term Frequency-Inverse Document Frequency\n",
    "\n",
    "**용어** \n",
    "- $tf(t, d)$: 특정 단어 $t$가 문서 $d$에서 등장한 횟수 (Term Frequency)\n",
    "- $df(t)$: 특정 단어 $t$가 등장한 문서의 수 (Document Frequency)\n",
    "- $N$: 전체 문서의 수\n",
    "\n",
    "**TF (Term Frequency)**\n",
    "- 단어 $t$의 문서 $d$에서의 빈도를 계산하는데, 가장 일반적인 방법은 해당 단어의 단순 빈도로 정의한다.\n",
    "\n",
    "$\n",
    "tf(t, d) = \\frac{\\text{단어 } t \\text{의 문서 } d \\text{ 내 등장 횟수}}{\\text{문서 } d \\text{의 전체 단어 수}}\n",
    "$\n",
    "\n",
    "**IDF (Inverse Document Frequency)**\n",
    "- 단어가 전체 문서에서 얼마나 중요한지를 계산한다.\n",
    "- 특정 단어가 많은 문서에서 등장하면, 이 단어는 중요도가 낮아진다. 이를 반영하기 위해 아래와 같은 식을 사용한다.\n",
    "\n",
    "$\n",
    "idf(t) = \\log\\left(\\frac{N}{1 + df(t)}\\right)\n",
    "$\n",
    "\n",
    "- 여기서 $1$을 더하는 이유는, 특정 단어가 모든 문서에 등장하지 않을 경우 $df(t) = 0$이 되어, 분모가 $0$이 되는 것을 방지하기 위함이다.\n",
    "  - 예를 들어, $\\log(5/(1+1))$과 $\\log(5/(1+2))$를 계산하면, 각각 $0.3979$와 $0.2218$이 된다.\n",
    "\n",
    "**TF-IDF 계산**\n",
    "- 위의 TF와 IDF를 결합하여 TF-IDF 가중치를 계산한다.\n",
    "\n",
    "$\n",
    "\\text{tf-idf}(t, d) = tf(t, d) \\times idf(t)\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9cfbda",
   "metadata": {},
   "source": [
    "**TfidfVectorizer의 주요 파라미터**\n",
    "<table border=\"1\" cellpadding=\"5\" cellspacing=\"0\">\n",
    "  <tr>\n",
    "    <th>Parameter</th>\n",
    "    <th>Description</th>\n",
    "    <th>Default Value</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><b>max_df</b></td>\n",
    "    <td>문서의 비율 값으로서, 해당 비율 이상 나타나는 단어를 무시한다. <br> 예를 들어, max_df=0.8이면, 80% 이상의 문서에서 나타나는 단어는 제외된다.</td>\n",
    "    <td>1.0</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><b>min_df</b></td>\n",
    "    <td>문서의 비율 값 또는 정수로, 해당 비율 이하 나타나는 단어를 무시한다. <br> 예를 들어, min_df=2이면, 두 개 이하의 문서에서만 나타나는 단어는 제외된다.</td>\n",
    "    <td>1</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><b>ngram_range</b></td>\n",
    "    <td>(min_n, max_n) 형식으로, 사용할 n-gram의 범위를 정의한다. <br> 예를 들어, (1, 2)로 설정하면 unigram과 bigram을 고려한다.</td>\n",
    "    <td>(1, 1)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>stop_words</td>\n",
    "    <td>불용어를 지정할 수 있다. \"english\"로 설정하면 영어 불용어를 사용한다.</td>\n",
    "    <td>None</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>max_features</td>\n",
    "    <td>벡터화할 때 고려할 최대 단어 수를 설정한다. 빈도순으로 상위 단어들이 선택된다.</td>\n",
    "    <td>None</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>use_idf</td>\n",
    "    <td>IDF(역문서 빈도)를 사용할지 여부를 지정한다. False로 설정하면 단순히 TF 값만 사용한다.</td>\n",
    "    <td>True</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>smooth_idf</td>\n",
    "    <td>IDF 계산 시, 0으로 나누는 것을 피하기 위해 추가적인 smoothing을 수행한다.</td>\n",
    "    <td>True</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>sublinear_tf</td>\n",
    "    <td>TF 값에 대해 sublinear scaling (1 + log(tf))를 적용할지 지정한다.</td>\n",
    "    <td>False</td>\n",
    "  </tr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "0329a996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.13847566 0.         0.13847566 0.         0.         0.\n",
      "  0.13847566 0.13847566 0.         0.         0.13847566 0.13847566\n",
      "  0.13847566 0.27695132 0.         0.13847566 0.         0.\n",
      "  0.09852657 0.13847566 0.27695132 0.13847566 0.13847566 0.13847566\n",
      "  0.41542698 0.13847566 0.13847566 0.         0.         0.\n",
      "  0.13847566 0.13847566 0.         0.         0.         0.\n",
      "  0.13847566 0.13847566 0.09852657 0.13847566 0.19705315 0.13847566\n",
      "  0.         0.         0.         0.13847566 0.13847566 0.\n",
      "  0.13847566 0.29557972 0.29557972]\n",
      " [0.         0.4473721  0.         0.11184302 0.22368605 0.11184302\n",
      "  0.         0.         0.11184302 0.11184302 0.         0.\n",
      "  0.         0.         0.11184302 0.         0.11184302 0.11184302\n",
      "  0.15915447 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.22368605 0.11184302 0.11184302\n",
      "  0.         0.         0.11184302 0.11184302 0.11184302 0.22368605\n",
      "  0.         0.         0.31830893 0.         0.07957723 0.\n",
      "  0.11184302 0.11184302 0.11184302 0.         0.         0.11184302\n",
      "  0.         0.55704063 0.07957723]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['all', 'and', 'around', 'bed', 'believe', 'blue', 'can', 'church',\n",
       "       'deep', 'ends', 'even', 'everywhere', 'feel', 'go', 'goes', 'here',\n",
       "       'hole', 'how', 'in', 'is', 'it', 'its', 'matrix', 'on', 'or',\n",
       "       'out', 'pay', 'pill', 'rabbit', 'red', 'room', 'see', 'show',\n",
       "       'stay', 'story', 'take', 'taxes', 'television', 'the', 'this',\n",
       "       'to', 'us', 'wake', 'want', 'whatever', 'when', 'window',\n",
       "       'wonderland', 'work', 'you', 'your'], dtype=object)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "texts_vecs = tfidf_vectorizer.fit_transform(texts)\n",
    "\n",
    "print(texts_vecs.toarray())\n",
    "tfidf_vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277a7206",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
